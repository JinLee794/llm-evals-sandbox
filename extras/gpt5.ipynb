{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure OpenAI: GPT-4o vs GPT-5 (Simple Demo)\n",
    "\n",
    "A minimal, Azure-only notebook showing:\n",
    "- How a baseline GPT-4o chat call looks (chat.completions).\n",
    "- How a GPT-5 (next-gen) Responses API call looks and why it’s simpler.\n",
    "- Small toggles: `reasoning_effort` and `verbosity` (when supported).\n",
    "\n",
    "Adjust your deployment names in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication Troubleshooting\n",
    "\n",
    "If you're getting a 401 authentication error, here are the most common fixes:\n",
    "\n",
    "1. **Check your environment variables** - Make sure these are set correctly:\n",
    "   - `V1_AZURE_OPENAI_ENDPOINT` - Should be your Azure OpenAI endpoint URL\n",
    "   - `AZURE_OPENAI_API_KEY` - Your API key from Azure portal\n",
    "\n",
    "2. **Verify endpoint format** - The endpoint should look like:\n",
    "   - `https://your-resource-name.openai.azure.com/openai/deployments/your-deployment-name/`\n",
    "   - Or for the v1 format: `https://your-resource-name.openai.azure.com/openai/v1`\n",
    "\n",
    "3. **Check deployment names** - Make sure `gpt-5` matches your actual deployment name in Azure\n",
    "\n",
    "4. **Try Azure AD authentication** instead of API key (more reliable for some setups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Azure AD authentication configured\n",
      "Endpoint: https://aifoundry825233136833-resource.cognitiveservices.azure.com/\n"
     ]
    }
   ],
   "source": [
    "# Alternative: Azure AD Authentication (recommended)\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "# This method uses Azure AD instead of API keys\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    token_provider = get_bearer_token_provider(\n",
    "        credential, \"https://cognitiveservices.azure.com/.default\"\n",
    "    )\n",
    "    \n",
    "    # Use AzureOpenAI client with proper endpoint format\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),  # https://your-resource.openai.azure.com/\n",
    "        azure_ad_token_provider=token_provider,\n",
    "        api_version=\"2024-12-01-preview\"  # Use latest API version that supports GPT-5\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Azure AD authentication configured\")\n",
    "    print(\"Endpoint:\", os.getenv(\"AZURE_OPENAI_ENDPOINT\"))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"❌ Azure AD auth failed:\", e)\n",
    "    print(\"Falling back to API key method...\")\n",
    "    \n",
    "    # Fallback to API key method\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        api_version=\"2024-12-01-preview\"\n",
    "    )\n",
    "    print(\"✅ API key authentication configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connection successful!\n",
      "Response: OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the connection with a simple call\n",
    "def test_connection():\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-5\",  # Replace with your actual deployment name\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Hello, can you respond with just 'OK'?\"}],\n",
    "            max_completion_tokens=1000\n",
    "        )\n",
    "        print(\"✅ Connection successful!\")\n",
    "        print(\"Response:\", response.choices[0].message.content)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"❌ Connection failed:\")\n",
    "        print(\"Error:\", str(e))\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "test_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Variables Setup\n",
    "\n",
    "Create a `.env` file in your project root with these variables:\n",
    "\n",
    "```bash\n",
    "# Azure OpenAI settings\n",
    "AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com/\n",
    "AZURE_OPENAI_API_KEY=your-api-key-here\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME=gpt-5  # or your actual deployment name\n",
    "\n",
    "# For v1 endpoint format (if using the OpenAI client)\n",
    "V1_AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com/openai/v1\n",
    "```\n",
    "\n",
    "**Common Issues:**\n",
    "- Make sure your Azure OpenAI resource has GPT-5 deployed\n",
    "- Verify the deployment name matches exactly (case-sensitive)\n",
    "- Check that your subscription has access to GPT-5 models\n",
    "- Ensure your API key hasn't expired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'code': '401', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m      6\u001b[0m token_provider \u001b[38;5;241m=\u001b[39m get_bearer_token_provider(\n\u001b[1;32m      7\u001b[0m     DefaultAzureCredential(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://cognitiveservices.azure.com/.default\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(  \n\u001b[1;32m     11\u001b[0m   base_url \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV1_AZURE_OPENAI_ENDPOINT\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     12\u001b[0m   api_key\u001b[38;5;241m=\u001b[39mtoken_provider,\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# replace with your model deployment name \u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat steps should I think about when writing my first Python API?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5000\u001b[39;49m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mmodel_dump_json(indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/openai/_utils/_utils.py:287\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:1147\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1145\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbosity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/openai/_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'code': '401', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from openai import OpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "token_provider = get_bearer_token_provider(\n",
    "    DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\"\n",
    ")\n",
    "\n",
    "client = OpenAI(  \n",
    "  base_url = os.getenv(\"V1_AZURE_OPENAI_ENDPOINT\"),\n",
    "  api_key=token_provider,\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-5\", # replace with your model deployment name \n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What steps should I think about when writing my first Python API?\"},\n",
    "    ],\n",
    "    max_completion_tokens = 5000\n",
    "\n",
    ")\n",
    "\n",
    "print(response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4o: Classic chat.completions\n",
    "Previous approach required us to give system prompts to control the behavior of the assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4o response:\n",
      "I’m available 24/7 to assist you!\n",
      "Latency (s): 1.656\n",
      "Tokens   : 59\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What are your hours of operation?\",\n",
    "    # \"How do I reset my password?\",\n",
    "    # \"Where can I find your pricing information?\",\n",
    "    # \"How do I contact support?\",\n",
    "    # \"How do I create an account?\",\n",
    "    # \"How can I update my billing information?\",\n",
    "    # \"What is your refund policy?\",\n",
    "    # \"How do I track my order?\",\n",
    "    # \"Can I change my subscription plan?\",\n",
    "    # \"How do I delete my account?\",\n",
    "    # \"Do you offer a free trial?\",\n",
    "    # \"How do I integrate with your API?\"\n",
    "]\n",
    "\n",
    "# Use the first question as the prompt for this example\n",
    "prompt = questions[0]\n",
    "\n",
    "started = time.time()\n",
    "resp_4o = client.chat.completions.create(\n",
    "    model=BASELINE_DEPLOYMENT,\n",
    "    messages=[\n",
    "        {\n",
    "            'role': \"system\",\n",
    "            'content': \"For simple questions, give brief answers. For complex technical issues, think step by step through the problem, consider multiple approaches, analyze each option...\",  # 200+ words of prompt hacks\n",
    "        },\n",
    "        {\n",
    "            'role':'user',\n",
    "            'content': prompt\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.7,\n",
    ")\n",
    "lat_4o = time.time() - started\n",
    "text_4o = resp_4o.choices[0].message.content\n",
    "usage_4o = resp_4o.usage\n",
    "\n",
    "print('GPT-4o response:')\n",
    "print(text_4o)\n",
    "print('Latency (s):', round(lat_4o,3))\n",
    "print('Tokens   :', getattr(usage_4o, 'total_tokens', None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-5: chat.completions\n",
    "Now with the `reasoning_effort` parameter, we can control the depth of the model's analysis and explanation without introducing additional system prompts or tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5 response:\n",
      "I’m available 24/7. If you’re asking about a specific business or location, tell me the name and city, and I’ll help you find their hours.\n",
      "Latency (s): 8.495\n",
      "Tokens   : 506\n"
     ]
    }
   ],
   "source": [
    "started = time.time()\n",
    "resp_5 = client.chat.completions.create(\n",
    "    model=PRIMARY_DEPLOYMENT,\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': prompt}\n",
    "    ],\n",
    "    reasoning_effort=\"high\",\n",
    ")\n",
    "\n",
    "lat_5 = time.time() - started\n",
    "text_5 = resp_5.choices[0].message.content\n",
    "usage_5 = resp_5.usage\n",
    "\n",
    "print('GPT-5 response:')\n",
    "print(text_5)\n",
    "print('Latency (s):', round(lat_5,3))\n",
    "print('Tokens   :', getattr(usage_5, 'total_tokens', None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimal reasoning runs GPT-5 with few or no reasoning tokens to minimize latency and speed up time-to-first-token. Use it for deterministic, lightweight tasks (extraction, formatting, short rewrites, simple classification) where explanations aren’t needed. If you don’t specify effort, it defaults to medium—set minimal explicitly when you want speed over deliberation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verbosity parameter\n",
    "The cells below demonstrate some of the new GPT-5 parameters and patterns from the OpenAI Cookbook:\n",
    "- `reasoning_effort`: controls how much chain-of-thought / internal reasoning effort the model should attempt\n",
    "- `verbosity`: controls how detailed the response should be\n",
    "- `Responses` API: the newer unified responses API (when available in your SDK) versus classic chat completions\n",
    "- A quick A/B comparison pattern to compare a baseline model vs your GPT-5 deployment\n",
    "\n",
    "These examples assume the `client`, `PRIMARY_DEPLOYMENT`, and `BASELINE_DEPLOYMENT` variables are defined earlier in the notebook (they are in the cells above). Adjust deployment names and params to match your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"how do I deploy a container to Azure?\"\n",
    "\n",
    "# Concise answer\n",
    "short_response = client.chat.completions.create(\n",
    "    model=PRIMARY_DEPLOYMENT,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    verbosity=\"low\"\n",
    ")\n",
    "\n",
    "# Balanced explanation  \n",
    "medium_response = client.chat.completions.create(\n",
    "    model=PRIMARY_DEPLOYMENT, \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    verbosity=\"medium\"\n",
    ")\n",
    "\n",
    "# Comprehensive guide\n",
    "detailed_response = client.chat.completions.create(\n",
    "    model=PRIMARY_DEPLOYMENT,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    verbosity=\"high\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Short ---\n",
      "Tokens: 4322\n",
      "Text preview: Great question. “Deploying a container to Azure” can mean a few different services depending on what you need. Here’s how to choose, plus quick step-by-step guides.\n",
      "\n",
      "Pick a target service\n",
      "- Azure Cont...\n",
      "\n",
      "--- Medium ---\n",
      "Tokens: 3948\n",
      "Text preview: Great question. “Deploy a container to Azure” can mean a few different services depending on what you need. Here’s a quick guide and copy-pasteable steps to get you running fast, plus options if you n...\n",
      "\n",
      "--- Detailed ---\n",
      "Tokens: 3735\n",
      "Text preview: Great question. “Deploy a container to Azure” can mean a few different services depending on what you need. Here’s how to pick, followed by quick step-by-step guides.\n",
      "\n",
      "Pick the right Azure service\n",
      "- E...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extractor to print latency & token counts (adapt names if different)\n",
    "def extract_text_and_tokens(resp_obj):\n",
    "    # SDK shapes vary; adjust the extraction below to match your openai client\n",
    "    try:\n",
    "        text = resp_obj.choices[0].message.content\n",
    "    except Exception:\n",
    "        # Try some other shapes\n",
    "        text = getattr(resp_obj, 'text', str(resp_obj))\n",
    "    tokens = None\n",
    "    try:\n",
    "        tokens = getattr(resp_obj, 'usage', None)\n",
    "        if tokens is not None:\n",
    "            tokens = getattr(tokens, 'total_tokens', tokens)\n",
    "    except Exception:\n",
    "        tokens = None\n",
    "    return text, tokens\n",
    "\n",
    "for name, resp, latency in [\n",
    "    (\"Short\", short_response, None),   # put measured latencies if you kept them\n",
    "    (\"Medium\", medium_response, None),\n",
    "    (\"Detailed\", detailed_response, None),\n",
    "]:\n",
    "    text, token_count = extract_text_and_tokens(resp)\n",
    "    # If you measured latency values (e.g., lat_short) replace `None` above and print them here\n",
    "    print(f'--- {name} ---')\n",
    "    print('Tokens:', token_count)\n",
    "    print('Text preview:', (text[:200] + '...') if isinstance(text, str) and len(text) > 200 else text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-5: Responses API - Additional Exercises (Cookbook examples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Verbosity Levels\n",
    "The verbosity parameter lets you hint the model to be more or less expansive in its replies.\n",
    "\n",
    "Values: \"low\", \"medium\", \"high\"\n",
    "\n",
    "- low → terse UX, minimal prose.\n",
    "- medium (default) → balanced detail.\n",
    "- high → verbose, great for audits, teaching, or hand-offs.\n",
    "Keep prompts stable and use the param rather than re-writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_fb207 th {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_fb207 td {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_fb207\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_fb207_level0_col0\" class=\"col_heading level0 col0\" >Verbosity</th>\n",
       "      <th id=\"T_fb207_level0_col1\" class=\"col_heading level0 col1\" >Sample Output</th>\n",
       "      <th id=\"T_fb207_level0_col2\" class=\"col_heading level0 col2\" >Output Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_fb207_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_fb207_row0_col0\" class=\"data row0 col0\" >low</td>\n",
       "      <td id=\"T_fb207_row0_col1\" class=\"data row0 col1\" >He found him in a cardboard box of quiet —\n",
       "a warm sliver of breath, a curl of ear,\n",
       "a clumsy paw that tapped the world like a question.\n",
       "The boy held him like a promise.\n",
       "\n",
       "They learned the names of morning together:\n",
       "puddle-splash, biscuit-mud, the small sun\n",
       "that follows sleep across the floor.\n",
       "A leash became a ribbon of trust,\n",
       "the backyard a kingdom of sudden discoveries.\n",
       "\n",
       "The dog learned to wait at the gate;\n",
       "the boy learned to wait for kindness.\n",
       "At dusk they traded secrets without words —\n",
       "a leaning shoulder, a crooked grin, a sigh\n",
       "that said everything about being unafraid.\n",
       "\n",
       "Years braided into each other the same way:\n",
       "games, storms, the soft hush of homework,\n",
       "and nights when the dog’s breathing slowed\n",
       "and the boy, now taller, kept his hand warm\n",
       "against the steady, faithful heartbeat of home.</td>\n",
       "      <td id=\"T_fb207_row0_col2\" class=\"data row0 col2\" >314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fb207_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_fb207_row1_col0\" class=\"data row1 col0\" >medium</td>\n",
       "      <td id=\"T_fb207_row1_col1\" class=\"data row1 col1\" >He came with a cardboard box of tremors and hope,\n",
       "a speckled shoulder that fit under my palm.\n",
       "For a week I measured the world by his tail—\n",
       "a small metronome that kept time with my heart.\n",
       "\n",
       "He taught me names for ordinary things:\n",
       "the way rain smells on pavement, the exact language of mud,\n",
       "how a stick could be treasure, a puddle a sea.\n",
       "He learned me, too, how to be patient—\n",
       "how to wait for crumbs to fall and for trust to arrive.\n",
       "\n",
       "We traded secrets: I in whispers, he in nudges,\n",
       "his breath a warm tide against my midnight fears.\n",
       "When thunder rolled like an engine on the roof,\n",
       "he climbed the ladder of my lap and held my knees.\n",
       "\n",
       "He chewed the edges of my homework, not out of malice\n",
       "but because he kept the small truth—that play is work too.\n",
       "He buried my socks like relics, guarded our doorway\n",
       "with an honest gravity that made the house a home.\n",
       "\n",
       "Afternoons were a map of sunlit routes—porch, stoop, the willow swing,\n",
       "the corner store where the clerk pretended not to know us.\n",
       "I learned to call him back by voice, by pocketful of biscuits,\n",
       "and sometimes by guilt when I had stayed too long at a friend's.\n",
       "\n",
       "Years braided themselves with his paws; he grew gray at the muzzle,\n",
       "I grew taller, my hand no longer fit his head quite the same.\n",
       "There were first goodbyes in smaller things: his favorite ball,\n",
       "the kitchen rug he always chose for naps, the quiet of our rooms.\n",
       "\n",
       "He never taught me to be brave in stories, only in small truths—\n",
       "to show up, to sit, to lean when other things fell apart.\n",
       "He taught me how to forgive, fast and clean as a tail’s sweep,\n",
       "how love is less about ceremony and more about presence.\n",
       "\n",
       "Sometimes now I close the door of the past and hear a faint jingle,\n",
       "a memory collar catching on the ribs of memory.\n",
       "I still set a place for lessons at the table:\n",
       "a bowl of patience, a leash of care, a pocket full of courage.\n",
       "\n",
       "In the photograph he always looks alive—tongue out, eyes open,\n",
       "and I am the same boy, forever learning to call him back.\n",
       "He was my first cartographer of the world,\n",
       "and in every park I pass, I still follow the path he taught me to trust.</td>\n",
       "      <td id=\"T_fb207_row1_col2\" class=\"data row1 col2\" >1005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fb207_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_fb207_row2_col0\" class=\"data row2 col0\" >high</td>\n",
       "      <td id=\"T_fb207_row2_col1\" class=\"data row2 col1\" >The day they brought him home, the dog fit in one palm —\n",
       "a warm, clumsy bundle of ears and unpracticed wags.\n",
       "The boy, with shoelaces still tied in loops of habit,\n",
       "held his breath as if he could stop time with quiet hands.\n",
       "They sat in the kitchen where sunlight made a slow map\n",
       "across the floor, and the dog sniffed for places to belong.\n",
       "\n",
       "At night the dog slept like a small, steady engine,\n",
       "a heartbeat pressed against the boy’s shins.\n",
       "Outside, the first real thunder startled both into silence,\n",
       "and the boy learned where to put his hands when fear came:\n",
       "on the soft, upturned belly, in the place that trusted him back.\n",
       "In the morning the dog discovered the world anew —\n",
       "mailboxes, puddles, the miraculous authority of squirrels —\n",
       "and the boy’s laugh sounded like permission for everything.\n",
       "\n",
       "Homework spread like a town of paper on the table,\n",
       "and the dog melted into the empty chair beside failures and triumphs.\n",
       "When the boy scraped his knee on a day bright as a promise,\n",
       "the dog licked the salt and left a tiny, glistening peace.\n",
       "They practiced the shape of being brave together:\n",
       "the boy learning to call, the dog learning to stay.\n",
       "They taught each other how to wait for little things —\n",
       "for thrown sticks, for visitors, for dinners —\n",
       "and how to answer the world with wagging insistence.\n",
       "\n",
       "Seasons rolled like slow pawprints across the land.\n",
       "Winter muffled the yard in a soft, forgetful white,\n",
       "and summer threw long, lazy shadows where they napped.\n",
       "The dog grew into his legs and the boy into his forehead,\n",
       "both tall enough to reach the top shelf of small adventures.\n",
       "Neighbors watched a pair that belonged entirely to itself —\n",
       "two silhouettes tipping at the edges of ordinary afternoons.\n",
       "\n",
       "Years folded like pages; collars were loosened, then tightened,\n",
       "and the dog’s muzzle grayed like a sky at dusk.\n",
       "One afternoon the boy—no longer quite a boy—sat on the porch,\n",
       "and the dog’s head found his knee as if it had always been home.\n",
       "Words were fewer now; there was a language in the touch,\n",
       "in the small, contented sigh that meant: I remember, too.\n",
       "When the last walk came, the road felt softer underfoot,\n",
       "each step sacred, each breath a small bright thing held between them.\n",
       "\n",
       "Afterward the house kept a silence like held breath,\n",
       "but the backyard still knew the ways they had loved.\n",
       "The boy, with older hands, dug a little hole and planted a bone\n",
       "of memory wrapped in forget-me-nots, and he laughed once,\n",
       "a sound that carried his grief and his gratitude in equal measure.\n",
       "Some nights he would wake and find the space by his feet empty,\n",
       "and there would be an ache like the absence of music.\n",
       "\n",
       "Time taught him what the dog had always known:\n",
       "how to meet the world with a wag, how to forgive the window’s ghosts,\n",
       "how to be present for the small, bright things.\n",
       "In the quiet that followed, the boy kept a pocket full of mornings —\n",
       "the scent of fur, the scrape of paws on tile, the thunk of a tail at the door —\n",
       "and he learned, simply, how to keep loving anything that loved him back.</td>\n",
       "      <td id=\"T_fb207_row2_col2\" class=\"data row2 col2\" >1053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x11e72c690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "question = \"Write a poem about a boy and his first pet dog.\"\n",
    "\n",
    "data = []\n",
    "\n",
    "for verbosity in [\"low\", \"medium\", \"high\"]:\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        input=question,\n",
    "        text={\"verbosity\": verbosity}\n",
    "    )\n",
    "\n",
    "    # Extract text\n",
    "    output_text = \"\"\n",
    "    for item in response.output:\n",
    "        if hasattr(item, \"content\") and item.content is not None:\n",
    "            for content in item.content:\n",
    "                if hasattr(content, \"text\"):\n",
    "                    output_text += content.text\n",
    "\n",
    "    usage = response.usage\n",
    "    data.append({\n",
    "        \"Verbosity\": verbosity,\n",
    "        \"Sample Output\": output_text,\n",
    "        \"Output Tokens\": usage.output_tokens\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display nicely with centered headers\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "styled_df = df.style.set_table_styles(\n",
    "    [\n",
    "        {'selector': 'th', 'props': [('text-align', 'center')]},  # Center column headers\n",
    "        {'selector': 'td', 'props': [('text-align', 'left')]}     # Left-align table cells\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(styled_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output tokens scale roughly linearly with verbosity: low (560) → medium (849) → high (1288).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Verbosity for Coding Use Cases\n",
    "The verbosity parameter also influences the length and complexity of generated code, as well as the depth of accompanying explanations. Here's an example, wherein we use various verboisty levels for a task to generate a Python program that sorts an array of 1000000 random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Verbosity: high\n",
      "Output:\n",
      "#!/usr/bin/env python3\n",
      "\"\"\"\n",
      "Generate an array of 1,000,000 random numbers and sort it, timing the operations.\n",
      "\n",
      "This script:\n",
      " - creates a list of 1_000_000 random floats in [0.0, 1.0)\n",
      " - sorts the list in-place using Python's Timsort (list.sort())\n",
      " - reports timings and a small sample of the sorted list\n",
      "\n",
      "Notes:\n",
      " - Python list of 1,000,000 float objects can use a significant amount of memory\n",
      "   (tens of MBs). If you have memory constraints or want faster sorting, consider\n",
      "   using numpy arrays (np.random.random and np.sort) which store data more compactly\n",
      "   and use optimized C code.\n",
      "\"\"\"\n",
      "\n",
      "import random\n",
      "import time\n",
      "import sys\n",
      "\n",
      "def main():\n",
      "    N = 1_000_000  # number of random numbers\n",
      "\n",
      "    print(f\"Generating {N} random floats...\")\n",
      "    t0 = time.perf_counter()\n",
      "    data = [random.random() for _ in range(N)]\n",
      "    t1 = time.perf_counter()\n",
      "    gen_time = t1 - t0\n",
      "    print(f\"Generation took {gen_time:.3f} seconds. (Approx memory used depends on Python implementation)\")\n",
      "\n",
      "    print(\"Sorting...\")\n",
      "    t0 = time.perf_counter()\n",
      "    data.sort()  # in-place sort using Timsort\n",
      "    t1 = time.perf_counter()\n",
      "    sort_time = t1 - t0\n",
      "    print(f\"Sorting took {sort_time:.3f} seconds.\")\n",
      "\n",
      "    # Quick validation and sample output\n",
      "    # Checking full sorted property is O(N) but cheap relative to sort time here\n",
      "    is_sorted = all(data[i] <= data[i+1] for i in range(len(data)-1))\n",
      "    print(f\"Array sorted? {is_sorted}\")\n",
      "\n",
      "    # Print a small sample of the sorted list\n",
      "    sample_count = 5\n",
      "    print(\"First few elements:\", data[:sample_count])\n",
      "    print(\"Last few elements: \", data[-sample_count:])\n",
      "\n",
      "    # Total time\n",
      "    print(f\"Total time: {gen_time + sort_time:.3f} seconds.\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "\n",
      "# Optional alternative (uncomment and install numpy if you prefer):\n",
      "# Using numpy usually uses less memory and sorts faster for large numeric arrays,\n",
      "# but requires the 'numpy' package.\n",
      "#\n",
      "# import numpy as np\n",
      "# def numpy_version():\n",
      "#     N = 1_000_000\n",
      "#     t0 = time.perf_counter()\n",
      "#     arr = np.random.random(N)  # dtype float64, stored compactly\n",
      "#     t1 = time.perf_counter()\n",
      "#     print(f\"NumPy generation took {t1-t0:.3f} s\")\n",
      "#     t0 = time.perf_counter()\n",
      "#     arr.sort()\n",
      "#     t1 = time.perf_counter()\n",
      "#     print(f\"NumPy sort took {t1-t0:.3f} s\")\n",
      "#     print(arr[:5], arr[-5:])\n",
      "Tokens => input: 21 | output: 1344\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Output a Python program that sorts an array of 1000000 random numbers\"\n",
    "\n",
    "def ask_with_verbosity(verbosity: str, question: str):\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        input=question,\n",
    "        text={\n",
    "            \"verbosity\": verbosity\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Extract assistant's text output\n",
    "    output_text = \"\"\n",
    "    for item in response.output:\n",
    "        if hasattr(item, \"content\") and item.content is not None:\n",
    "            for content in item.content:\n",
    "                if hasattr(content, \"text\"):\n",
    "                    output_text += content.text\n",
    "\n",
    "    # Token usage details\n",
    "    usage = response.usage\n",
    "\n",
    "    print(\"--------------------------------\")\n",
    "    print(f\"Verbosity: {verbosity}\")\n",
    "    print(\"Output:\")\n",
    "    print(output_text)\n",
    "    print(\"Tokens => input: {} | output: {}\".format(\n",
    "        usage.input_tokens, usage.output_tokens\n",
    "    ))\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "ask_with_verbosity(\"high\", prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes and next steps:\n",
    "- If `client.responses.create` is not available in your installed SDK, adapt the cell to use `client.chat.completions.create` or upgrade the `openai` package used in this environment.\n",
    "- Try different `reasoning_effort` and `verbosity` settings to see how the model trade-offs change between concision and depth.\n",
    "- For tool integrations, the Cookbook shows patterns for registering and invoking external tools; add a small local tool (e.g., calculator) and adapt the request to call it to practice a safe tool flow.\n",
    "- When running these cells in shared or public repos, avoid committing prompts that contain explicit or actionable harmful content; keep tests in local, secured environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Free-Form Function Calling\n",
    "GPT‑5 can now send raw text payloads - anything from Python scripts to SQL queries - to your custom tool without wrapping the data in JSON using the new tool \"type\": \"custom\". This differs from classic structured function calls, giving you greater flexibility when interacting with external runtimes such as:\n",
    "\n",
    "- code_exec with sandboxes (Python, C++, Java, …)\n",
    "- SQL databases\n",
    "- Shell environments\n",
    "- Configuration generators\n",
    "\n",
    "> Note that custom tool type does NOT support parallel tool calling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Start Example - Compute the Area of a Circle\n",
    "The code below produces a simple python code to calculate area of a circle, and instruct the model to use the freeform tool call to output the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4040905876.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[93], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    tool_choice_options = ToolChoice.\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from openai.types.responses import ToolChoice, ToolChoiceAllowedParam\n",
    "\n",
    "tool_choice_options = ToolChoiceAllowedParam(\n",
    "    name=\"tool_choice\",\n",
    "    description=\"Select a tool to use\",\n",
    "    choices=[\n",
    "        ToolChoice(name=\"calculator\", description=\"Performs mathematical calculations\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=\"Please use the code_exec tool to calculate the area of a circle with radius equal to the number of 'r's in strawberry\",\n",
    "    text={\"format\": {\"type\": \"text\"}},\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"custom\",\n",
    "            \"name\": \"code_exec\",\n",
    "            \"description\": \"Executes arbitrary python code\",\n",
    "        }\n",
    "    ]\n",
    "    tool_choice=tool_choice_options\n",
    ")\n",
    "print(response.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model emits a tool call containing raw Python. You execute that code server‑side, capture the printed result, and send it back in a follow‑up responses.create call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Benchmark - Sorting an Array in Three Languages\n",
    "To illustrate the use of free form tool calling, we will ask GPT‑5 to:\n",
    "\n",
    "Generate Python, C++, and Java code that sorts a fixed array 10 times.\n",
    "Print only the time (in ms) taken for each iteration in the code.\n",
    "Call all three functions, and then stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- tool name ---\n",
      "code_exec_python\n",
      "--- tool call argument (generated code) ---\n",
      "arr = [448, 986, 255, 884, 632, 623, 246, 439, 936, 925, 644, 159, 777, 986, 706, 723, 534, 862, 195, 686, 846, 880, 970, 276, 613, 736, 329, 622, 870, 284, 945, 708, 267, 327, 678, 807, 687, 890, 907, 645, 364, 333, 385, 262, 730, 603, 945, 358, 923, 930, 761, 504, 870, 561, 517, 928, 994, 949, 233, 137, 670, 555, 149, 870, 997, 809, 180, 498, 914, 508, 411, 378, 394, 368, 766, 486, 757, 319, 338, 159, 585, 934, 654, 194, 542, 188, 934, 163, 889, 736, 792, 737, 667, 772, 198, 971, 459, 402, 989, 949]\n",
      "\n",
      "import time\n",
      "\n",
      "start = time.perf_counter()\n",
      "for _ in range(10):\n",
      "    a = arr[:]  # copy\n",
      "    a.sort()\n",
      "elapsed_ms = int((time.perf_counter() - start) * 1000)\n",
      "print(elapsed_ms)\n",
      "--- tool name ---\n",
      "code_exec_cpp\n",
      "--- tool call argument (generated code) ---\n",
      "#include <algorithm>\n",
      "#include <vector>\n",
      "#include <chrono>\n",
      "#include <iostream>\n",
      "using namespace std;\n",
      "\n",
      "int main() {\n",
      "    vector<int> arr = {448, 986, 255, 884, 632, 623, 246, 439, 936, 925, 644, 159, 777, 986, 706, 723, 534, 862, 195, 686, 846, 880, 970, 276, 613, 736, 329, 622, 870, 284, 945, 708, 267, 327, 678, 807, 687, 890, 907, 645, 364, 333, 385, 262, 730, 603, 945, 358, 923, 930, 761, 504, 870, 561, 517, 928, 994, 949, 233, 137, 670, 555, 149, 870, 997, 809, 180, 498, 914, 508, 411, 378, 394, 368, 766, 486, 757, 319, 338, 159, 585, 934, 654, 194, 542, 188, 934, 163, 889, 736, 792, 737, 667, 772, 198, 971, 459, 402, 989, 949};\n",
      "    auto start = chrono::high_resolution_clock::now();\n",
      "    for (int i = 0; i < 10; ++i) {\n",
      "        auto a = arr;\n",
      "        sort(a.begin(), a.end());\n",
      "    }\n",
      "    auto end = chrono::high_resolution_clock::now();\n",
      "    auto ms = chrono::duration_cast<chrono::milliseconds>(end - start).count();\n",
      "    cout << ms;\n",
      "    return 0;\n",
      "}\n",
      "--- tool name ---\n",
      "code_exec_java\n",
      "--- tool call argument (generated code) ---\n",
      "import java.util.Arrays;\n",
      "\n",
      "public class Main {\n",
      "    public static void main(String[] args) {\n",
      "        int[] arr = new int[] {448, 986, 255, 884, 632, 623, 246, 439, 936, 925, 644, 159, 777, 986, 706, 723, 534, 862, 195, 686, 846, 880, 970, 276, 613, 736, 329, 622, 870, 284, 945, 708, 267, 327, 678, 807, 687, 890, 907, 645, 364, 333, 385, 262, 730, 603, 945, 358, 923, 930, 761, 504, 870, 561, 517, 928, 994, 949, 233, 137, 670, 555, 149, 870, 997, 809, 180, 498, 914, 508, 411, 378, 394, 368, 766, 486, 757, 319, 338, 159, 585, 934, 654, 194, 542, 188, 934, 163, 889, 736, 792, 737, 667, 772, 198, 971, 459, 402, 989, 949};\n",
      "        long start = System.nanoTime();\n",
      "        for (int i = 0; i < 10; i++) {\n",
      "            int[] a = Arrays.copyOf(arr, arr.length);\n",
      "            Arrays.sort(a);\n",
      "        }\n",
      "        long end = System.nanoTime();\n",
      "        long ms = (end - start) / 1_000_000;\n",
      "        System.out.print(ms);\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "MODEL_NAME = \"gpt-5\"\n",
    "\n",
    "# Tools that will be passed to every model invocation. They are defined once so\n",
    "# that the configuration lives in a single place.\n",
    "TOOLS = [\n",
    "    {\n",
    "        \"type\": \"custom\",\n",
    "        \"name\": \"code_exec_python\",\n",
    "        \"description\": \"Executes python code\",\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"custom\",\n",
    "        \"name\": \"code_exec_cpp\",\n",
    "        \"description\": \"Executes c++ code\",\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"custom\",\n",
    "        \"name\": \"code_exec_java\",\n",
    "        \"description\": \"Executes java code\",\n",
    "    },\n",
    "]\n",
    "\n",
    "def create_response(\n",
    "    input_messages: List[dict],\n",
    "    previous_response_id: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"Wrapper around ``client.responses.create``.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_messages: List[dict]\n",
    "        The running conversation history to feed to the model.\n",
    "    previous_response_id: str | None\n",
    "        Pass the ``response.id`` from the *previous* call so the model can keep\n",
    "        the thread of the conversation.  Omit on the very first request.\n",
    "    \"\"\"\n",
    "    kwargs = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"input\": input_messages,\n",
    "        \"text\": {\"format\": {\"type\": \"text\"}},\n",
    "        \"tools\": TOOLS,\n",
    "    }\n",
    "    if previous_response_id:\n",
    "        kwargs[\"previous_response_id\"] = previous_response_id\n",
    "\n",
    "    return client.responses.create(**kwargs)\n",
    "\n",
    "# Recursive \n",
    "def run_conversation(\n",
    "    input_messages: List[dict],\n",
    "    previous_response_id: Optional[str] = None,\n",
    "):\n",
    "  \n",
    "    response = create_response(input_messages, previous_response_id)\n",
    "\n",
    "    # ``response.output`` is expected to be a list where element 0 is the model\n",
    "    # message.  Element 1 (if present) denotes a tool call.  When the model is\n",
    "    # done with tool calls, that element is omitted.\n",
    "    tool_call = response.output[1] if len(response.output) > 1 else None\n",
    "\n",
    "    if tool_call and tool_call.type == \"custom_tool_call\":\n",
    "        print(\"--- tool name ---\")\n",
    "        print(tool_call.name)\n",
    "        print(\"--- tool call argument (generated code) ---\")\n",
    "        print(tool_call.input)\n",
    "        \n",
    "        # Add a synthetic *tool result* so the model can continue the thread.\n",
    "        \n",
    "        input_messages.append(\n",
    "            {\n",
    "                \"type\": \"function_call_output\",\n",
    "                \"call_id\": tool_call.call_id,\n",
    "                \"output\": \"done\", # <-- replace with the result of the tool call\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Recurse with updated conversation and track the response id so the\n",
    "        # model is aware of the prior turn.\n",
    "        return run_conversation(input_messages, previous_response_id=response.id)\n",
    "    else:\n",
    "        # Base-case: no further tool call - return. \n",
    "        return \n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "Write code to sort the array of numbers in three languages: C++, Python and Java (10 times each)using code_exec functions.\n",
    "\n",
    "ALWAYS CALL THESE THREE FUNCTIONS EXACTLY ONCE: code_exec_python, code_exec_cpp and code_exec_java tools to sort the array in each language. Stop once you've called these three functions in each language once.\n",
    "\n",
    "Print only the time it takes to sort the array in milliseconds. \n",
    "\n",
    "[448, 986, 255, 884, 632, 623, 246, 439, 936, 925, 644, 159, 777, 986, 706, 723, 534, 862, 195, 686, 846, 880, 970, 276, 613, 736, 329, 622, 870, 284, 945, 708, 267, 327, 678, 807, 687, 890, 907, 645, 364, 333, 385, 262, 730, 603, 945, 358, 923, 930, 761, 504, 870, 561, 517, 928, 994, 949, 233, 137, 670, 555, 149, 870, 997, 809, 180, 498, 914, 508, 411, 378, 394, 368, 766, 486, 757, 319, 338, 159, 585, 934, 654, 194, 542, 188, 934, 163, 889, 736, 792, 737, 667, 772, 198, 971, 459, 402, 989, 949]\n",
    "\"\"\"\n",
    "\n",
    "# Initial developer message.\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"developer\",\n",
    "        \"content\": prompt,\n",
    "    }\n",
    "]\n",
    "\n",
    "run_conversation(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model output three code blocks in Python, C++ and Java for the same algorithm. The output of the function call was chained back into the model as input to allow model to keep going until all the functions have been called exactly once.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Context‑Free Grammar (CFG)\n",
    "### Overview\n",
    "A context‑free grammar is a collection of production rules that define which strings belong to a language. Each rule rewrites a non‑terminal symbol into a sequence of terminals (literal tokens) and/or other non‑terminals, independent of surrounding context—hence context‑free. CFGs can capture the syntax of most programming languages and, in OpenAI custom tools, serve as contracts that force the model to emit only strings that the grammar accepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammar Fundamentals\n",
    "\n",
    "Supported Grammar Syntax\n",
    "\n",
    "- Lark - https://lark-parser.readthedocs.io/en/stable/\n",
    "- Regex - https://docs.rs/regex/latest/regex/#syntax\n",
    "We use LLGuidance under the hood to constrain model sampling: https://github.com/guidance-ai/llguidance.\n",
    "\n",
    "Unsupported Lark Features\n",
    "\n",
    "- Lookaround in regexes ((?=...), (?!...), etc.)\n",
    "- Lazy modifier (*?, +?, ??) in regexes.\n",
    "- Terminal priorities, templates, %declares, %import (except %import common).\n",
    "- Terminals vs Rules & Greedy Lexing\n",
    "\n",
    "| Concept | Take-away |\n",
    "|---|---|\n",
    "| Terminals (UPPER) | Matched first by the lexer — longest match wins. |\n",
    "| Rules (lower) | Combine terminals; cannot influence how text is tokenised. |\n",
    "| Greedy lexer | Never try to “shape” free text across multiple terminals — you’ll lose control. |\n",
    "\n",
    "** Correct vs Incorrect Pattern Design\n",
    "\n",
    "**✅ One bounded terminal handles free‑text between anchors**\n",
    "\n",
    "start: SENTENCE\n",
    "\n",
    "SENTENCE: /[A-Za-z, ](the hero|a dragon)[A-Za-z, ](fought|saved)[A-Za-z, ](a treasure|the kingdom)[A-Za-z, ]./\n",
    "\n",
    "**❌ Don’t split free‑text across multiple terminals/rules**\n",
    "\n",
    "start: sentence\n",
    "\n",
    "sentence: /[A-Za-z, ]+/ subject /[A-Za-z, ]+/ verb /[A-Za-z, ]+/ object /[A-Za-z, ]+/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - SQL Dialect — MS SQL vs PostgreSQL\n",
    "The following code example is now the canonical reference for building multi‑dialect SQL tools with CFGs. It demonstrates:\n",
    "\n",
    "Two isolated grammar definitions (mssql_grammar_definition, postgres_grammar_definition) encoding TOP vs LIMIT semantics.\n",
    "How to prompt, invoke, and inspect tool calls in a single script.\n",
    "A side‑by‑side inspection of the assistant’s responses.\n",
    "Define the LARK grammars for different SQL dialects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "# ----------------- grammars for MS SQL dialect -----------------\n",
    "mssql_grammar = textwrap.dedent(r\"\"\"\n",
    "            // ---------- Punctuation & operators ----------\n",
    "            SP: \" \"\n",
    "            COMMA: \",\"\n",
    "            GT: \">\"\n",
    "            EQ: \"=\"\n",
    "            SEMI: \";\"\n",
    "\n",
    "            // ---------- Start ----------\n",
    "            start: \"SELECT\" SP \"TOP\" SP NUMBER SP select_list SP \"FROM\" SP table SP \"WHERE\" SP amount_filter SP \"AND\" SP date_filter SP \"ORDER\" SP \"BY\" SP sort_cols SEMI\n",
    "\n",
    "            // ---------- Projections ----------\n",
    "            select_list: column (COMMA SP column)*\n",
    "            column: IDENTIFIER\n",
    "\n",
    "            // ---------- Tables ----------\n",
    "            table: IDENTIFIER\n",
    "\n",
    "            // ---------- Filters ----------\n",
    "            amount_filter: \"total_amount\" SP GT SP NUMBER\n",
    "            date_filter: \"order_date\" SP GT SP DATE\n",
    "\n",
    "            // ---------- Sorting ----------\n",
    "            sort_cols: \"order_date\" SP \"DESC\"\n",
    "\n",
    "            // ---------- Terminals ----------\n",
    "            IDENTIFIER: /[A-Za-z_][A-Za-z0-9_]*/\n",
    "            NUMBER: /[0-9]+/\n",
    "            DATE: /'[0-9]{4}-[0-9]{2}-[0-9]{2}'/\n",
    "    \"\"\")\n",
    "\n",
    "# ----------------- grammars for PostgreSQL dialect -----------------\n",
    "postgres_grammar = textwrap.dedent(r\"\"\"\n",
    "            // ---------- Punctuation & operators ----------\n",
    "            SP: \" \"\n",
    "            COMMA: \",\"\n",
    "            GT: \">\"\n",
    "            EQ: \"=\"\n",
    "            SEMI: \";\"\n",
    "\n",
    "            // ---------- Start ----------\n",
    "            start: \"SELECT\" SP select_list SP \"FROM\" SP table SP \"WHERE\" SP amount_filter SP \"AND\" SP date_filter SP \"ORDER\" SP \"BY\" SP sort_cols SP \"LIMIT\" SP NUMBER SEMI\n",
    "\n",
    "            // ---------- Projections ----------\n",
    "            select_list: column (COMMA SP column)*\n",
    "            column: IDENTIFIER\n",
    "\n",
    "            // ---------- Tables ----------\n",
    "            table: IDENTIFIER\n",
    "\n",
    "            // ---------- Filters ----------\n",
    "            amount_filter: \"total_amount\" SP GT SP NUMBER\n",
    "            date_filter: \"order_date\" SP GT SP DATE\n",
    "\n",
    "            // ---------- Sorting ----------\n",
    "            sort_cols: \"order_date\" SP \"DESC\"\n",
    "\n",
    "            // ---------- Terminals ----------\n",
    "            IDENTIFIER: /[A-Za-z_][A-Za-z0-9_]*/\n",
    "            NUMBER: /[0-9]+/\n",
    "            DATE: /'[0-9]{4}-[0-9]{2}-[0-9]{2}'/\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate specific SQL dialect\n",
    "Let's define the prompt, and call the function to produce MS SQL dialect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MS SQL Query ---\n",
      "SELECT TOP 5 customer_id, order_id, order_date, total_amount FROM orders WHERE total_amount > 500 AND order_date > '2025-01-01' ORDER BY order_date DESC;\n"
     ]
    }
   ],
   "source": [
    "sql_prompt_mssql = (\n",
    "    \"Call the mssql_grammar to generate a query for Microsoft SQL Server that retrieve the \"\n",
    "    \"five most recent orders per customer, showing customer_id, order_id, order_date, and total_amount, \"\n",
    "    \"where total_amount > 500 and order_date is after '2025-01-01'. \"\n",
    ")\n",
    "\n",
    "response_mssql = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=sql_prompt_mssql,\n",
    "    text={\"format\": {\"type\": \"text\"}},\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"custom\",\n",
    "            \"name\": \"mssql_grammar\",\n",
    "            \"description\": \"Executes read-only Microsoft SQL Server queries limited to SELECT statements with TOP and basic WHERE/ORDER BY. YOU MUST REASON HEAVILY ABOUT THE QUERY AND MAKE SURE IT OBEYS THE GRAMMAR.\",\n",
    "            \"format\": {\n",
    "                \"type\": \"grammar\",\n",
    "                \"syntax\": \"lark\",\n",
    "                \"definition\": mssql_grammar\n",
    "            }\n",
    "        },\n",
    "    ],\n",
    "    parallel_tool_calls=False\n",
    ")\n",
    "\n",
    "print(\"--- MS SQL Query ---\")\n",
    "print(response_mssql.output[1].input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output SQL accurately uses \"SELECT TOP\" construct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_prompt_pg = (\n",
    "    \"Call the postgres_grammar to generate a query for PostgreSQL that retrieve the \"\n",
    "    \"five most recent orders per customer, showing customer_id, order_id, order_date, and total_amount, \"\n",
    "    \"where total_amount > 500 and order_date is after '2025-01-01'. \"\n",
    ")\n",
    "\n",
    "response_pg = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=sql_prompt_pg,\n",
    "    text={\"format\": {\"type\": \"text\"}},\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"custom\",\n",
    "            \"name\": \"postgres_grammar\",\n",
    "            \"description\": \"Executes read-only PostgreSQL queries limited to SELECT statements with LIMIT and basic WHERE/ORDER BY. YOU MUST REASON HEAVILY ABOUT THE QUERY AND MAKE SURE IT OBEYS THE GRAMMAR.\",\n",
    "            \"format\": {\n",
    "                \"type\": \"grammar\",\n",
    "                \"syntax\": \"lark\",\n",
    "                \"definition\": postgres_grammar\n",
    "            }\n",
    "        },\n",
    "    ],\n",
    "    parallel_tool_calls=False,\n",
    ")\n",
    "\n",
    "print(\"--- PG SQL Query ---\")\n",
    "print(response_pg.output[1].input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Dialect | Generated Query | Key Difference |\n",
    "|---|---|---|\n",
    "| MS SQL Server | `SELECT TOP 5 customer_id, order_id, order_date, total_amount FROM orders WHERE total_amount > 500 AND order_date > '2025-01-01' ORDER BY order_date DESC;` | Uses `TOP N` clause placed immediately after `SELECT` (before column list). |\n",
    "| PostgreSQL | `SELECT customer_id, order_id, order_date, total_amount FROM orders WHERE total_amount > 500 AND order_date > '2025-01-01' ORDER BY order_date DESC LIMIT 5;` | Uses `LIMIT N` appended after `ORDER BY`. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - Regex CFG Syntax\n",
    "The following code example demonstrates using the Regex CFG syntax to constrain the freeform tool call to a certain timestamp pattern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Timestamp ---\n",
      "2025-08-07 10:00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "timestamp_grammar_definition = r\"^\\d{4}-(0[1-9]|1[0-2])-(0[1-9]|[12]\\d|3[01]) (?:[01]\\d|2[0-3]):[0-5]\\d$\"\n",
    "\n",
    "timestamp_prompt = (\n",
    "        \"Call the timestamp_grammar to save a timestamp for August 7th 2025 at 10AM.\"\n",
    ")\n",
    "\n",
    "response_mssql = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=timestamp_prompt,\n",
    "    text={\"format\": {\"type\": \"text\"}},\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"custom\",\n",
    "            \"name\": \"timestamp_grammar\",\n",
    "            \"description\": \"Saves a timestamp in date + time in 24-hr format.\",\n",
    "            \"format\": {\n",
    "                \"type\": \"grammar\",\n",
    "                \"syntax\": \"regex\",\n",
    "                \"definition\": timestamp_grammar_definition\n",
    "            }\n",
    "        },\n",
    "    ],\n",
    "    parallel_tool_calls=False\n",
    ")\n",
    "\n",
    "print(\"--- Timestamp ---\")\n",
    "print(response_mssql.output[1].input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "Lark grammars can be tricky to perfect. While simple grammars perform most reliably, complex grammars often require iteration on the grammar definition itself, the prompt, and the tool description to ensure that the model does not go out of distribution.\n",
    "\n",
    "- **Keep terminals bounded** – use `/[^.\\n]{0,10}*\\./` rather than `/.*\\./`. Limit matches both by content (negated character class) and by length (`{M,N}` quantifier).\n",
    "- **Prefer explicit char‑classes over `.` wildcards.**\n",
    "- **Thread whitespace explicitly**, e.g. using `SP = \" \"`, instead of a global `%ignore`.\n",
    "- **Describe your tool**: tell the model exactly what the CFG accepts and instruct it to reason heavily about compliance.\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "- **API rejects the grammar because it is too complex** ➜ Simplify rules and terminals, remove `%ignore.*`.\n",
    "- **Unexpected tokens** ➜ Confirm terminals aren't overlapping; check greedy lexer.\n",
    "- **When the model drifts \"out‑of‑distribution\"** (shows up as the model producing excessively long or repetitive outputs, it is syntactically valid but is semantically wrong):\n",
    "    - Tighten the grammar.\n",
    "    - Iterate on the prompt (add few-shot examples) and tool description (explain the grammar and instruct the model to reason to conform to it).\n",
    "    - Experiment with a higher reasoning effort (e.g, bump from medium to high).\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- [Lark Docs](https://lark-parser.readthedocs.io/en/stable/)\n",
    "- [Lark IDE](https://www.lark-parser.org/ide/)\n",
    "- [LLGuidance Syntax](https://github.com/guidance-ai/llguidance/blob/main/docs/syntax.md)\n",
    "- [Regex (Rust crate)](https://docs.rs/regex/latest/regex/#syntax)\n",
    "\n",
    "## 3.6 Takeaways\n",
    "\n",
    "Context-Free Grammar (CFG) support in GPT-5 lets you strictly constrain model output to match predefined syntax, ensuring only valid strings are generated. This is especially useful for enforcing programming language rules or custom formats, reducing post-processing and errors. By providing a precise grammar and clear tool description, you can make the model reliably stay within your target output structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
