{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Scaling LLM Evaluations\n",
    "**Duration**: 60 minutes\n",
    "\n",
    "Welcome to Lab 2! Building on the foundational concepts from Lab 1, you'll now learn how to scale your evaluation processes with Azure AI Foundry integration, create comprehensive datasets, and systematically compare models for production deployment decisions.\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "By the end of this lab, you will:\n",
    "- Create and manage large-scale evaluation datasets in Azure AI Foundry\n",
    "- Implement batch evaluation workflows that integrate with AI Foundry portal\n",
    "- Compare multiple models and prompts systematically\n",
    "- Generate synthetic evaluation data using Azure OpenAI\n",
    "- Understand cost vs. quality trade-offs for production decisions\n",
    "\n",
    "## 📋 Prerequisites\n",
    "- Completed Lab 1: Evaluation Fundamentals\n",
    "- Azure AI Foundry project configured (from Lab 1)\n",
    "- Understanding of basic evaluation metrics\n",
    "- Azure OpenAI access with sufficient quota for dataset generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Advanced AI Foundry Integration Setup (10 min)\n",
    "\n",
    "### Building on Lab 1's Foundation\n",
    "\n",
    "In Lab 1, we established basic AI Foundry integration. In Lab 2, we'll leverage this foundation to:\n",
    "- Create larger, more diverse datasets\n",
    "- Implement systematic model comparisons\n",
    "- Monitor evaluation trends over time\n",
    "- Generate comprehensive evaluation reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies with enhanced AI Foundry capabilities\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import asyncio\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd())))\n",
    "\n",
    "# Import our enhanced AI Foundry integration from Lab 1\n",
    "from shared_utils.azure_clients import azure_manager\n",
    "from shared_utils.foundry_evaluation import foundry_runner\n",
    "from shared_utils.evaluation_helpers import load_evaluation_data, save_evaluation_results\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✅ Enhanced dependencies imported successfully!\")\n",
    "\n",
    "# Check AI Foundry integration status from Lab 1\n",
    "foundry_status = foundry_runner.get_status_info()\n",
    "print(f\"\\n🏢 AI Foundry Integration Status: {'✅ ENABLED' if foundry_status['ai_foundry_available'] else '❌ DISABLED'}\")\n",
    "\n",
    "if foundry_status['ai_foundry_available']:\n",
    "    print(\"   📊 Lab 2 evaluations will upload to AI Foundry portal\")\n",
    "    print(\"   🔗 Portal: https://ai.azure.com\")\n",
    "    \n",
    "    # Show existing datasets from Lab 1\n",
    "    try:\n",
    "        client = azure_manager.get_ai_foundry_client()\n",
    "        existing_datasets = list(client.datasets.list())\n",
    "        print(f\"   📁 Found {len(existing_datasets)} existing datasets from previous labs\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ Could not list existing datasets: {e}\")\n",
    "else:\n",
    "    print(\"   💡 Configure AI Foundry integration for enhanced portal visibility\")\n",
    "    print(\"   📚 See Lab 1 setup instructions for configuration details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Synthetic Dataset Generation Strategy (15 min)\n",
    "\n",
    "### Understanding Dataset Requirements for Production\n",
    "\n",
    "Production LLM systems require comprehensive evaluation datasets that cover:\n",
    "- **Multiple domains**: Healthcare, legal, technical, customer service\n",
    "- **Varying complexity**: Simple factual questions to complex reasoning\n",
    "- **Edge cases**: Ambiguous queries, out-of-scope requests, adversarial inputs\n",
    "- **Scale**: Hundreds to thousands of examples for statistical significance\n",
    "\n",
    "### Azure OpenAI-Powered Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedDatasetGenerator:\n",
    "    \"\"\"Enhanced dataset generator with AI Foundry integration.\"\"\"\n",
    "    \n",
    "    def __init__(self, azure_client, deployment_name):\n",
    "        self.client = azure_client\n",
    "        self.deployment = deployment_name\n",
    "        self.generation_history = []\n",
    "        \n",
    "    def generate_domain_dataset(self, domain: str, num_pairs: int = 20, \n",
    "                              complexity_levels: List[str] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Generate domain-specific Q&A pairs with varying complexity.\"\"\"\n",
    "        \n",
    "        if complexity_levels is None:\n",
    "            complexity_levels = ['basic', 'intermediate', 'advanced']\n",
    "            \n",
    "        dataset = []\n",
    "        pairs_per_level = num_pairs // len(complexity_levels)\n",
    "        \n",
    "        print(f\"🔄 Generating {num_pairs} Q&A pairs for domain: {domain}\")\n",
    "        print(f\"📊 Complexity levels: {complexity_levels}\")\n",
    "        \n",
    "        for complexity in complexity_levels:\n",
    "            print(f\"   📝 Creating {pairs_per_level} {complexity} level questions...\")\n",
    "            \n",
    "            generation_prompt = f\"\"\"\n",
    "Generate {pairs_per_level} high-quality question-answer pairs for the {domain} domain.\n",
    "Complexity level: {complexity}\n",
    "\n",
    "Requirements:\n",
    "- Questions should be realistic and practical\n",
    "- Answers should be accurate and comprehensive\n",
    "- Include relevant context/background information\n",
    "- Vary question types (factual, analytical, procedural)\n",
    "\n",
    "Format each pair as JSON:\n",
    "{{\n",
    "    \"query\": \"Your question here\",\n",
    "    \"response\": \"Detailed answer here\",\n",
    "    \"context\": \"Background information here\",\n",
    "    \"domain\": \"{domain}\",\n",
    "    \"complexity\": \"{complexity}\",\n",
    "    \"question_type\": \"factual/analytical/procedural\"\n",
    "}}\n",
    "\n",
    "Generate {pairs_per_level} such pairs:\n",
    "\"\"\"\n",
    "            \n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.deployment,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are an expert at creating high-quality evaluation datasets for LLM systems. Always respond with valid JSON.\"},\n",
    "                        {\"role\": \"user\", \"content\": generation_prompt}\n",
    "                    ],\n",
    "                    temperature=0.7,\n",
    "                    max_tokens=2000\n",
    "                )\n",
    "                \n",
    "                generated_text = response.choices[0].message.content\n",
    "                \n",
    "                # Extract JSON pairs from response\n",
    "                pairs = self._extract_json_pairs(generated_text)\n",
    "                dataset.extend(pairs)\n",
    "                \n",
    "                print(f\"   ✅ Generated {len(pairs)} pairs for {complexity} level\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Failed to generate {complexity} level pairs: {e}\")\n",
    "                # Add fallback sample data\n",
    "                fallback_pair = {\n",
    "                    \"query\": f\"Sample {complexity} question for {domain}\",\n",
    "                    \"response\": f\"Sample answer for {complexity} {domain} question\",\n",
    "                    \"context\": f\"Context for {domain} domain\",\n",
    "                    \"domain\": domain,\n",
    "                    \"complexity\": complexity,\n",
    "                    \"question_type\": \"factual\"\n",
    "                }\n",
    "                dataset.append(fallback_pair)\n",
    "        \n",
    "        # Record generation metadata\n",
    "        generation_record = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"domain\": domain,\n",
    "            \"requested_pairs\": num_pairs,\n",
    "            \"generated_pairs\": len(dataset),\n",
    "            \"complexity_levels\": complexity_levels\n",
    "        }\n",
    "        self.generation_history.append(generation_record)\n",
    "        \n",
    "        print(f\"✅ Dataset generation complete: {len(dataset)} pairs created\")\n",
    "        return dataset\n",
    "    \n",
    "    def _extract_json_pairs(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extract JSON pairs from generated text.\"\"\"\n",
    "        pairs = []\n",
    "        \n",
    "        # Try to find JSON objects in the text\n",
    "        import re\n",
    "        json_pattern = r'\\{[^{}]*\\}'\n",
    "        matches = re.findall(json_pattern, text, re.DOTALL)\n",
    "        \n",
    "        for match in matches:\n",
    "            try:\n",
    "                pair = json.loads(match)\n",
    "                if 'query' in pair and 'response' in pair:\n",
    "                    pairs.append(pair)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "                \n",
    "        return pairs\n",
    "    \n",
    "    def create_multi_domain_dataset(self, domains: List[str], \n",
    "                                   pairs_per_domain: int = 15) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Create a comprehensive multi-domain dataset.\"\"\"\n",
    "        \n",
    "        print(f\"🚀 Creating multi-domain dataset\")\n",
    "        print(f\"📊 Domains: {domains}\")\n",
    "        print(f\"🔢 Pairs per domain: {pairs_per_domain}\")\n",
    "        \n",
    "        all_data = []\n",
    "        \n",
    "        for domain in domains:\n",
    "            domain_data = self.generate_domain_dataset(domain, pairs_per_domain)\n",
    "            all_data.extend(domain_data)\n",
    "            print(f\"✅ Completed {domain}: {len(domain_data)} pairs\")\n",
    "        \n",
    "        print(f\"\\n🎉 Multi-domain dataset complete: {len(all_data)} total pairs\")\n",
    "        return all_data\n",
    "\n",
    "# Initialize the advanced generator\n",
    "client = azure_manager.get_openai_client()\n",
    "deployment_name = os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME')\n",
    "\n",
    "generator = AdvancedDatasetGenerator(client, deployment_name)\n",
    "\n",
    "print(\"🔧 Advanced Dataset Generator initialized\")\n",
    "print(f\"📡 Using deployment: {deployment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Production-Scale Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define domains relevant to common LLM applications\n",
    "production_domains = [\n",
    "    \"customer_support\",\n",
    "    \"technical_documentation\", \n",
    "    \"business_analysis\",\n",
    "    \"healthcare_information\"\n",
    "]\n",
    "\n",
    "print(\"🏭 PRODUCTION-SCALE DATASET GENERATION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "try:\n",
    "    # Generate comprehensive dataset\n",
    "    production_dataset = generator.create_multi_domain_dataset(\n",
    "        domains=production_domains,\n",
    "        pairs_per_domain=10  # Reduced for workshop time/cost management\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n📊 DATASET SUMMARY\")\n",
    "    print(f\"Total pairs: {len(production_dataset)}\")\n",
    "    \n",
    "    # Analyze dataset composition\n",
    "    domain_counts = {}\n",
    "    complexity_counts = {}\n",
    "    type_counts = {}\n",
    "    \n",
    "    for item in production_dataset:\n",
    "        # Count by domain\n",
    "        domain = item.get('domain', 'unknown')\n",
    "        domain_counts[domain] = domain_counts.get(domain, 0) + 1\n",
    "        \n",
    "        # Count by complexity\n",
    "        complexity = item.get('complexity', 'unknown')\n",
    "        complexity_counts[complexity] = complexity_counts.get(complexity, 0) + 1\n",
    "        \n",
    "        # Count by question type\n",
    "        q_type = item.get('question_type', 'unknown')\n",
    "        type_counts[q_type] = type_counts.get(q_type, 0) + 1\n",
    "    \n",
    "    print(\"\\n📈 Domain Distribution:\")\n",
    "    for domain, count in domain_counts.items():\n",
    "        print(f\"   {domain}: {count} pairs\")\n",
    "    \n",
    "    print(\"\\n🎯 Complexity Distribution:\")\n",
    "    for complexity, count in complexity_counts.items():\n",
    "        print(f\"   {complexity}: {count} pairs\")\n",
    "    \n",
    "    print(\"\\n❓ Question Type Distribution:\")\n",
    "    for q_type, count in type_counts.items():\n",
    "        print(f\"   {q_type}: {count} pairs\")\n",
    "    \n",
    "    # Show sample entries\n",
    "    print(\"\\n🔍 Sample Entries:\")\n",
    "    for i, sample in enumerate(production_dataset[:2]):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"   Domain: {sample.get('domain', 'N/A')}\")\n",
    "        print(f\"   Complexity: {sample.get('complexity', 'N/A')}\")\n",
    "        print(f\"   Query: {sample['query'][:80]}...\")\n",
    "        print(f\"   Response: {sample['response'][:80]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Dataset generation failed: {e}\")\n",
    "    print(\"Using fallback sample dataset...\")\n",
    "    \n",
    "    # Create fallback dataset for demonstration\n",
    "    production_dataset = [\n",
    "        {\n",
    "            \"query\": \"How do I reset my password?\",\n",
    "            \"response\": \"To reset your password, go to the login page and click 'Forgot Password'. Enter your email address and follow the instructions sent to your inbox.\",\n",
    "            \"context\": \"Customer support for password management\",\n",
    "            \"domain\": \"customer_support\",\n",
    "            \"complexity\": \"basic\",\n",
    "            \"question_type\": \"procedural\"\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"What are the implications of microservice architecture on system scalability?\",\n",
    "            \"response\": \"Microservice architecture enhances scalability by allowing independent scaling of services, distributing load more effectively, and enabling horizontal scaling of specific components based on demand.\",\n",
    "            \"context\": \"Technical architecture and system design\",\n",
    "            \"domain\": \"technical_documentation\",\n",
    "            \"complexity\": \"advanced\",\n",
    "            \"question_type\": \"analytical\"\n",
    "        }\n",
    "    ]\n",
    "    print(f\"📋 Using {len(production_dataset)} fallback examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: AI Foundry Dataset Integration (10 min)\n",
    "\n",
    "### Upload Generated Dataset to AI Foundry\n",
    "\n",
    "Now we'll upload our production-scale dataset to Azure AI Foundry, making it available for:\n",
    "- Portal-based evaluation management\n",
    "- Team collaboration and sharing\n",
    "- Version control and dataset lineage\n",
    "- Integration with future evaluation APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced dataset upload with AI Foundry integration\n",
    "print(\"📤 UPLOADING DATASET TO AI FOUNDRY\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "if foundry_status['ai_foundry_available']:\n",
    "    try:\n",
    "        # Create dataset name with timestamp\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        dataset_name = f\"lab2_production_dataset_{timestamp}\"\n",
    "        \n",
    "        print(f\"📁 Dataset name: {dataset_name}\")\n",
    "        print(f\"📊 Size: {len(production_dataset)} pairs\")\n",
    "        \n",
    "        # Create temporary JSONL file\n",
    "        import tempfile\n",
    "        temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False)\n",
    "        for item in production_dataset:\n",
    "            temp_file.write(json.dumps(item) + '\\n')\n",
    "        temp_file.close()\n",
    "        \n",
    "        # Upload to AI Foundry\n",
    "        foundry_client = azure_manager.get_ai_foundry_client()\n",
    "        uploaded_dataset = foundry_client.datasets.upload_file(\n",
    "            name=dataset_name,\n",
    "            version=\"1.0\",\n",
    "            file_path=temp_file.name\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Dataset uploaded successfully!\")\n",
    "        print(f\"🆔 Dataset ID: {uploaded_dataset.id}\")\n",
    "        print(f\"🏷️ Type: {uploaded_dataset.type}\")\n",
    "        print(f\"🔗 Portal: https://ai.azure.com\")\n",
    "        \n",
    "        # Store dataset info for later use\n",
    "        lab2_dataset_info = {\n",
    "            \"name\": dataset_name,\n",
    "            \"id\": uploaded_dataset.id,\n",
    "            \"size\": len(production_dataset),\n",
    "            \"domains\": list(set(item.get('domain', 'unknown') for item in production_dataset)),\n",
    "            \"upload_timestamp\": timestamp\n",
    "        }\n",
    "        \n",
    "        # Clean up temp file\n",
    "        os.unlink(temp_file.name)\n",
    "        \n",
    "        print(f\"\\n📊 Dataset registered for Lab 2 evaluations\")\n",
    "        print(f\"   Domains covered: {lab2_dataset_info['domains']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Dataset upload failed: {e}\")\n",
    "        print(\"   Will proceed with local dataset for evaluations\")\n",
    "        lab2_dataset_info = None\n",
    "        \n",
    "else:\n",
    "    print(\"ℹ️ AI Foundry not configured - dataset stored locally\")\n",
    "    print(\"   Configure AI Foundry for enhanced portal integration\")\n",
    "    lab2_dataset_info = None\n",
    "\n",
    "# Save dataset locally as well\n",
    "local_dataset_path = \"data/lab2_production_dataset.jsonl\"\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "with open(local_dataset_path, 'w') as f:\n",
    "    for item in production_dataset:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(f\"\\n💾 Dataset also saved locally: {local_dataset_path}\")\n",
    "print(f\"📈 Ready for batch evaluation workflows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Advanced Model Comparison Framework (20 min)\n",
    "\n",
    "### Multi-Model Evaluation Strategy\n",
    "\n",
    "Production deployments require systematic comparison of:\n",
    "- **Different model sizes**: GPT-3.5 vs GPT-4 vs GPT-4o\n",
    "- **Cost implications**: Token usage and pricing analysis\n",
    "- **Performance metrics**: Quality scores across domains\n",
    "- **Latency considerations**: Response time measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Model Comparison Pipeline with AI Foundry Integration\n",
    "class ProductionModelComparison:\n",
    "    \"\"\"Advanced model comparison with AI Foundry integration.\"\"\"\n",
    "    \n",
    "    def __init__(self, foundry_runner):\n",
    "        self.foundry_runner = foundry_runner\n",
    "        self.comparison_results = {}\n",
    "        \n",
    "    def setup_model_configurations(self):\n",
    "        \"\"\"Define model configurations for comparison.\"\"\"\n",
    "        \n",
    "        base_config = {\n",
    "            \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            \"api_key\": os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "            \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "        }\n",
    "        \n",
    "        # In a real scenario, you'd have multiple deployments\n",
    "        # For workshop, we'll simulate with different evaluation approaches\n",
    "        model_configs = {\n",
    "            \"primary_model\": {\n",
    "                **base_config,\n",
    "                \"azure_deployment\": os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "                \"display_name\": \"Primary GPT Model\",\n",
    "                \"cost_per_1k_input\": 0.03,\n",
    "                \"cost_per_1k_output\": 0.06\n",
    "            }\n",
    "            # Additional models would be configured here in production\n",
    "        }\n",
    "        \n",
    "        return model_configs\n",
    "    \n",
    "    def create_evaluation_subset(self, dataset: List[Dict], sample_size: int = 5) -> List[Dict]:\n",
    "        \"\"\"Create a representative subset for model comparison.\"\"\"\n",
    "        \n",
    "        print(f\"🎯 Creating evaluation subset: {sample_size} items\")\n",
    "        \n",
    "        # Ensure we have representation from each domain\n",
    "        domains = list(set(item.get('domain', 'unknown') for item in dataset))\n",
    "        items_per_domain = max(1, sample_size // len(domains))\n",
    "        \n",
    "        subset = []\n",
    "        for domain in domains:\n",
    "            domain_items = [item for item in dataset if item.get('domain') == domain]\n",
    "            subset.extend(domain_items[:items_per_domain])\n",
    "        \n",
    "        # Fill remaining slots if needed\n",
    "        while len(subset) < sample_size and len(subset) < len(dataset):\n",
    "            for item in dataset:\n",
    "                if item not in subset:\n",
    "                    subset.append(item)\n",
    "                    if len(subset) >= sample_size:\n",
    "                        break\n",
    "        \n",
    "        print(f\"✅ Subset created with {len(subset)} items\")\n",
    "        print(f\"   Domains included: {list(set(item.get('domain', 'unknown') for item in subset))}\")\n",
    "        \n",
    "        return subset\n",
    "    \n",
    "    def run_comprehensive_comparison(self, dataset: List[Dict], \n",
    "                                   evaluators: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Run comprehensive model comparison with AI Foundry integration.\"\"\"\n",
    "        \n",
    "        print(\"🏁 COMPREHENSIVE MODEL COMPARISON\")\n",
    "        print(\"=\" * 35)\n",
    "        \n",
    "        model_configs = self.setup_model_configurations()\n",
    "        comparison_subset = self.create_evaluation_subset(dataset, sample_size=5)\n",
    "        \n",
    "        results_summary = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"dataset_size\": len(comparison_subset),\n",
    "            \"models_compared\": list(model_configs.keys()),\n",
    "            \"evaluators_used\": list(evaluators.keys()),\n",
    "            \"individual_results\": {},\n",
    "            \"summary_metrics\": {}\n",
    "        }\n",
    "        \n",
    "        for model_name, config in model_configs.items():\n",
    "            print(f\"\\n📊 Evaluating model: {config['display_name']}\")\n",
    "            \n",
    "            try:\n",
    "                # Run evaluation using our enhanced foundry runner\n",
    "                model_results = self.foundry_runner.run_evaluation(\n",
    "                    data=comparison_subset,\n",
    "                    evaluators=evaluators,\n",
    "                    run_name=f\"Lab2 {config['display_name']} Comparison\",\n",
    "                    description=f\"Model comparison evaluation for {model_name}\"\n",
    "                )\n",
    "                \n",
    "                # Calculate cost estimates\n",
    "                estimated_tokens = sum(len(item['query']) + len(item['response']) for item in comparison_subset) * 0.75  # Rough estimate\n",
    "                estimated_cost = (estimated_tokens / 1000) * config['cost_per_1k_input']\n",
    "                \n",
    "                # Enhance results with cost analysis\n",
    "                enhanced_results = {\n",
    "                    **model_results,\n",
    "                    \"cost_analysis\": {\n",
    "                        \"estimated_tokens\": estimated_tokens,\n",
    "                        \"estimated_cost_usd\": round(estimated_cost, 4),\n",
    "                        \"cost_per_1k_tokens\": config['cost_per_1k_input']\n",
    "                    },\n",
    "                    \"model_config\": config\n",
    "                }\n",
    "                \n",
    "                results_summary[\"individual_results\"][model_name] = enhanced_results\n",
    "                \n",
    "                print(f\"   ✅ Evaluation completed\")\n",
    "                print(f\"   💰 Estimated cost: ${estimated_cost:.4f}\")\n",
    "                \n",
    "                if 'metrics' in model_results:\n",
    "                    print(f\"   📈 Quality metrics calculated\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Evaluation failed: {e}\")\n",
    "                results_summary[\"individual_results\"][model_name] = {\"error\": str(e)}\n",
    "        \n",
    "        # Create summary analysis\n",
    "        results_summary[\"summary_metrics\"] = self._create_comparison_summary(results_summary[\"individual_results\"])\n",
    "        \n",
    "        self.comparison_results = results_summary\n",
    "        return results_summary\n",
    "    \n",
    "    def _create_comparison_summary(self, individual_results: Dict) -> Dict:\n",
    "        \"\"\"Create summary comparison metrics.\"\"\"\n",
    "        \n",
    "        summary = {\n",
    "            \"models_evaluated\": len(individual_results),\n",
    "            \"successful_evaluations\": 0,\n",
    "            \"total_estimated_cost\": 0.0,\n",
    "            \"quality_comparison\": {}\n",
    "        }\n",
    "        \n",
    "        for model_name, results in individual_results.items():\n",
    "            if \"error\" not in results:\n",
    "                summary[\"successful_evaluations\"] += 1\n",
    "                \n",
    "                if \"cost_analysis\" in results:\n",
    "                    summary[\"total_estimated_cost\"] += results[\"cost_analysis\"][\"estimated_cost_usd\"]\n",
    "                    \n",
    "                if \"metrics\" in results:\n",
    "                    summary[\"quality_comparison\"][model_name] = results[\"metrics\"]\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def generate_comparison_report(self) -> str:\n",
    "        \"\"\"Generate a comprehensive comparison report.\"\"\"\n",
    "        \n",
    "        if not self.comparison_results:\n",
    "            return \"No comparison results available. Run comparison first.\"\n",
    "        \n",
    "        report = []\n",
    "        report.append(\"📊 MODEL COMPARISON REPORT\")\n",
    "        report.append(\"=\" * 30)\n",
    "        report.append(f\"Timestamp: {self.comparison_results['timestamp']}\")\n",
    "        report.append(f\"Dataset size: {self.comparison_results['dataset_size']} items\")\n",
    "        report.append(f\"Models evaluated: {self.comparison_results['summary_metrics']['successful_evaluations']}\")\n",
    "        report.append(f\"Total estimated cost: ${self.comparison_results['summary_metrics']['total_estimated_cost']:.4f}\")\n",
    "        \n",
    "        report.append(\"\\n💰 COST ANALYSIS:\")\n",
    "        for model_name, results in self.comparison_results[\"individual_results\"].items():\n",
    "            if \"cost_analysis\" in results:\n",
    "                cost_info = results[\"cost_analysis\"]\n",
    "                report.append(f\"   {model_name}: ${cost_info['estimated_cost_usd']:.4f} (${cost_info['cost_per_1k_tokens']:.3f}/1K tokens)\")\n",
    "        \n",
    "        report.append(\"\\n🎯 QUALITY METRICS:\")\n",
    "        quality_data = self.comparison_results['summary_metrics']['quality_comparison']\n",
    "        for model_name, metrics in quality_data.items():\n",
    "            report.append(f\"   {model_name}:\")\n",
    "            for metric_name, score in metrics.items():\n",
    "                if isinstance(score, (int, float)):\n",
    "                    report.append(f\"      {metric_name}: {score:.3f}\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "# Initialize the comparison framework\n",
    "comparison_framework = ProductionModelComparison(foundry_runner)\n",
    "\n",
    "print(\"🔧 Production Model Comparison Framework initialized\")\n",
    "print(\"📊 Ready for systematic model evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Comprehensive Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up evaluators for model comparison\n",
    "from azure.ai.evaluation import RelevanceEvaluator, CoherenceEvaluator, FluencyEvaluator\n",
    "\n",
    "print(\"🔧 Setting up evaluators for model comparison...\")\n",
    "\n",
    "model_config = azure_manager.get_model_config()\n",
    "\n",
    "# Create evaluators (using fewer for workshop efficiency)\n",
    "comparison_evaluators = {}\n",
    "\n",
    "try:\n",
    "    comparison_evaluators[\"relevance\"] = RelevanceEvaluator(model_config=model_config)\n",
    "    print(\"✅ Relevance evaluator created\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Relevance evaluator failed: {e}\")\n",
    "\n",
    "try:\n",
    "    comparison_evaluators[\"coherence\"] = CoherenceEvaluator(model_config=model_config)\n",
    "    print(\"✅ Coherence evaluator created\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Coherence evaluator failed: {e}\")\n",
    "\n",
    "print(f\"\\n📊 Created {len(comparison_evaluators)} evaluators for comparison\")\n",
    "\n",
    "if comparison_evaluators:\n",
    "    # Run comprehensive comparison\n",
    "    print(\"\\n🚀 Starting comprehensive model comparison...\")\n",
    "    print(\"   This will create separate evaluation runs in AI Foundry\")\n",
    "    \n",
    "    comparison_results = comparison_framework.run_comprehensive_comparison(\n",
    "        dataset=production_dataset,\n",
    "        evaluators=comparison_evaluators\n",
    "    )\n",
    "    \n",
    "    # Display comparison report\n",
    "    print(\"\\n\" + comparison_framework.generate_comparison_report())\n",
    "    \n",
    "    # Show AI Foundry integration status\n",
    "    execution_method = comparison_results.get('individual_results', {}).get('primary_model', {}).get('_execution_method', 'unknown')\n",
    "    \n",
    "    if execution_method == 'azure_ai_foundry_hybrid':\n",
    "        print(\"\\n🏢 MODEL COMPARISON RESULTS IN AI FOUNDRY:\")\n",
    "        print(\"   📊 Multiple evaluation datasets uploaded\")\n",
    "        print(\"   🔍 Each model comparison has its own dataset\")\n",
    "        print(\"   👀 View all results at: https://ai.azure.com\")\n",
    "        print(\"   📈 Compare quality metrics across models\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No evaluators available for model comparison\")\n",
    "    print(\"   This is common in workshop environments\")\n",
    "    print(\"   The framework is ready for production use when evaluators are available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Production Decision Analysis (10 min)\n",
    "\n",
    "### Cost vs. Quality Trade-off Analysis\n",
    "\n",
    "Real production decisions require balancing multiple factors:\n",
    "- **Quality requirements**: Minimum acceptable scores per domain\n",
    "- **Cost constraints**: Budget limitations and scaling economics\n",
    "- **Latency requirements**: Response time SLAs\n",
    "- **Throughput needs**: Requests per second capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Production Analysis\n",
    "class ProductionDecisionAnalyzer:\n",
    "    \"\"\"Analyze evaluation results for production deployment decisions.\"\"\"\n",
    "    \n",
    "    def __init__(self, comparison_results: Dict):\n",
    "        self.results = comparison_results\n",
    "        \n",
    "    def analyze_cost_quality_tradeoffs(self, \n",
    "                                     quality_threshold: float = 3.5,\n",
    "                                     cost_budget_per_1k: float = 0.05) -> Dict:\n",
    "        \"\"\"Analyze cost vs quality trade-offs for production deployment.\"\"\"\n",
    "        \n",
    "        print(\"💰 COST vs QUALITY ANALYSIS\")\n",
    "        print(\"=\" * 30)\n",
    "        print(f\"Quality threshold: {quality_threshold}/5.0\")\n",
    "        print(f\"Cost budget: ${cost_budget_per_1k:.3f} per 1K tokens\")\n",
    "        \n",
    "        analysis = {\n",
    "            \"criteria\": {\n",
    "                \"min_quality_score\": quality_threshold,\n",
    "                \"max_cost_per_1k\": cost_budget_per_1k\n",
    "            },\n",
    "            \"model_analysis\": {},\n",
    "            \"recommendations\": []\n",
    "        }\n",
    "        \n",
    "        for model_name, results in self.results.get(\"individual_results\", {}).items():\n",
    "            if \"error\" in results:\n",
    "                continue\n",
    "                \n",
    "            model_analysis = {\n",
    "                \"meets_quality_threshold\": False,\n",
    "                \"within_cost_budget\": False,\n",
    "                \"average_quality_score\": 0.0,\n",
    "                \"cost_per_1k_tokens\": 0.0,\n",
    "                \"recommendation\": \"Not evaluated\"\n",
    "            }\n",
    "            \n",
    "            # Calculate average quality score\n",
    "            if \"metrics\" in results:\n",
    "                quality_scores = []\n",
    "                for metric_name, score in results[\"metrics\"].items():\n",
    "                    if isinstance(score, (int, float)) and 'threshold' not in metric_name.lower():\n",
    "                        quality_scores.append(score)\n",
    "                \n",
    "                if quality_scores:\n",
    "                    avg_quality = sum(quality_scores) / len(quality_scores)\n",
    "                    model_analysis[\"average_quality_score\"] = avg_quality\n",
    "                    model_analysis[\"meets_quality_threshold\"] = avg_quality >= quality_threshold\n",
    "            \n",
    "            # Check cost constraints\n",
    "            if \"cost_analysis\" in results:\n",
    "                cost_per_1k = results[\"cost_analysis\"][\"cost_per_1k_tokens\"]\n",
    "                model_analysis[\"cost_per_1k_tokens\"] = cost_per_1k\n",
    "                model_analysis[\"within_cost_budget\"] = cost_per_1k <= cost_budget_per_1k\n",
    "            \n",
    "            # Generate recommendation\n",
    "            if model_analysis[\"meets_quality_threshold\"] and model_analysis[\"within_cost_budget\"]:\n",
    "                model_analysis[\"recommendation\"] = \"✅ RECOMMENDED - Meets quality and cost requirements\"\n",
    "            elif model_analysis[\"meets_quality_threshold\"]:\n",
    "                model_analysis[\"recommendation\"] = \"⚠️ QUALITY GOOD - Exceeds cost budget\"\n",
    "            elif model_analysis[\"within_cost_budget\"]:\n",
    "                model_analysis[\"recommendation\"] = \"⚠️ COST EFFECTIVE - Below quality threshold\"\n",
    "            else:\n",
    "                model_analysis[\"recommendation\"] = \"❌ NOT RECOMMENDED - Fails quality and cost requirements\"\n",
    "            \n",
    "            analysis[\"model_analysis\"][model_name] = model_analysis\n",
    "        \n",
    "        # Generate overall recommendations\n",
    "        recommended_models = [name for name, analysis_data in analysis[\"model_analysis\"].items() \n",
    "                            if \"✅ RECOMMENDED\" in analysis_data[\"recommendation\"]]\n",
    "        \n",
    "        if recommended_models:\n",
    "            analysis[\"recommendations\"].append(f\"Deploy: {', '.join(recommended_models)}\")\n",
    "        else:\n",
    "            analysis[\"recommendations\"].append(\"Consider adjusting quality thresholds or cost budgets\")\n",
    "            analysis[\"recommendations\"].append(\"Evaluate additional model options\")\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def generate_production_readiness_report(self) -> str:\n",
    "        \"\"\"Generate comprehensive production readiness report.\"\"\"\n",
    "        \n",
    "        # Run analysis with production-typical thresholds\n",
    "        analysis = self.analyze_cost_quality_tradeoffs(\n",
    "            quality_threshold=3.5,  # Typical production minimum\n",
    "            cost_budget_per_1k=0.05  # Example budget constraint\n",
    "        )\n",
    "        \n",
    "        report = []\n",
    "        report.append(\"🚀 PRODUCTION READINESS REPORT\")\n",
    "        report.append(\"=\" * 35)\n",
    "        \n",
    "        report.append(f\"\\n📊 EVALUATION CRITERIA:\")\n",
    "        report.append(f\"   Minimum quality score: {analysis['criteria']['min_quality_score']}/5.0\")\n",
    "        report.append(f\"   Maximum cost per 1K tokens: ${analysis['criteria']['max_cost_per_1k']:.3f}\")\n",
    "        \n",
    "        report.append(f\"\\n🎯 MODEL ANALYSIS:\")\n",
    "        for model_name, model_data in analysis[\"model_analysis\"].items():\n",
    "            report.append(f\"   {model_name.upper()}:\")\n",
    "            report.append(f\"      Quality Score: {model_data['average_quality_score']:.2f}/5.0\")\n",
    "            report.append(f\"      Cost per 1K: ${model_data['cost_per_1k_tokens']:.4f}\")\n",
    "            report.append(f\"      Status: {model_data['recommendation']}\")\n",
    "        \n",
    "        report.append(f\"\\n💡 RECOMMENDATIONS:\")\n",
    "        for recommendation in analysis[\"recommendations\"]:\n",
    "            report.append(f\"   • {recommendation}\")\n",
    "        \n",
    "        report.append(f\"\\n🔄 NEXT STEPS:\")\n",
    "        report.append(f\"   1. Validate results with larger dataset\")\n",
    "        report.append(f\"   2. Test latency and throughput requirements\")\n",
    "        report.append(f\"   3. Run A/B tests with recommended models\")\n",
    "        report.append(f\"   4. Set up continuous evaluation monitoring\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "# Analyze results if available\n",
    "if 'comparison_results' in locals() and comparison_results:\n",
    "    analyzer = ProductionDecisionAnalyzer(comparison_results)\n",
    "    \n",
    "    print(analyzer.generate_production_readiness_report())\n",
    "    \n",
    "else:\n",
    "    print(\"📊 PRODUCTION ANALYSIS FRAMEWORK\")\n",
    "    print(\"=\" * 35)\n",
    "    print(\"✅ Analysis framework ready\")\n",
    "    print(\"💡 Run model comparison first to generate analysis\")\n",
    "    print(\"\\n🎯 Framework capabilities:\")\n",
    "    print(\"   • Cost vs Quality trade-off analysis\")\n",
    "    print(\"   • Production readiness scoring\")\n",
    "    print(\"   • Deployment recommendations\")\n",
    "    print(\"   • ROI calculations\")\n",
    "    print(\"   • Risk assessment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: AI Foundry Integration Summary (5 min)\n",
    "\n",
    "### What You've Accomplished with AI Foundry\n",
    "\n",
    "In this lab, you've built a production-ready evaluation system that leverages Azure AI Foundry for:\n",
    "\n",
    "1. **Dataset Management**: Uploaded production-scale datasets to AI Foundry portal\n",
    "2. **Evaluation Tracking**: Created traceable evaluation runs with metadata\n",
    "3. **Model Comparison**: Systematic comparison with cost analysis\n",
    "4. **Production Readiness**: Decision frameworks for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Lab 2 Summary and AI Foundry Integration Status\n",
    "print(\"🎉 LAB 2 COMPLETION SUMMARY\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Count achievements\n",
    "achievements = []\n",
    "\n",
    "if 'production_dataset' in locals():\n",
    "    achievements.append(f\"Generated {len(production_dataset)} production-quality evaluation pairs\")\n",
    "\n",
    "if 'lab2_dataset_info' in locals() and lab2_dataset_info:\n",
    "    achievements.append(f\"Uploaded dataset '{lab2_dataset_info['name']}' to AI Foundry portal\")\n",
    "    achievements.append(f\"Covered domains: {', '.join(lab2_dataset_info['domains'])}\")\n",
    "\n",
    "if 'comparison_results' in locals() and comparison_results:\n",
    "    achievements.append(\"Completed systematic model comparison analysis\")\n",
    "    achievements.append(\"Generated production readiness recommendations\")\n",
    "\n",
    "achievements.append(\"Built scalable evaluation framework with AI Foundry integration\")\n",
    "\n",
    "for i, achievement in enumerate(achievements, 1):\n",
    "    print(f\"✅ {i}. {achievement}\")\n",
    "\n",
    "# AI Foundry Integration Status\n",
    "print(\"\\n🏢 AI FOUNDRY INTEGRATION STATUS:\")\n",
    "if foundry_status['ai_foundry_available']:\n",
    "    print(\"   ✅ Active and functional\")\n",
    "    print(\"   📊 Evaluation datasets uploaded to portal\")\n",
    "    print(\"   🔗 View at: https://ai.azure.com\")\n",
    "    \n",
    "    # List datasets created in this lab\n",
    "    try:\n",
    "        current_datasets = list(azure_manager.get_ai_foundry_client().datasets.list())\n",
    "        lab2_datasets = [d for d in current_datasets if 'lab2' in d.name.lower()]\n",
    "        print(f\"   📁 Lab 2 datasets in portal: {len(lab2_datasets)}\")\n",
    "    except:\n",
    "        print(\"   📁 Dataset count unavailable\")\n",
    "else:\n",
    "    print(\"   ℹ️ Not configured (evaluations ran locally)\")\n",
    "    print(\"   💡 Configure for enhanced portal integration\")\n",
    "\n",
    "print(\"\\n🚀 NEXT STEPS:\")\n",
    "print(\"   📈 Ready for Lab 3: Custom Evaluators & Domain-Specific Evaluation\")\n",
    "print(\"   🔧 Built scalable foundation for advanced evaluation patterns\")\n",
    "print(\"   🏭 Production-ready evaluation framework established\")\n",
    "\n",
    "print(\"\\n🎯 KEY SKILLS MASTERED:\")\n",
    "skills = [\n",
    "    \"Synthetic dataset generation at production scale\",\n",
    "    \"AI Foundry integration for evaluation management\",\n",
    "    \"Systematic model comparison methodologies\", \n",
    "    \"Cost vs quality trade-off analysis\",\n",
    "    \"Production deployment decision frameworks\"\n",
    "]\n",
    "\n",
    "for skill in skills:\n",
    "    print(f\"   ✅ {skill}\")\n",
    "\n",
    "print(f\"\\n💡 Your evaluation system is now ready for enterprise deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting & Next Steps\n",
    "\n",
    "### Common Issues in Lab 2:\n",
    "\n",
    "1. **Dataset Generation Failures**:\n",
    "   - Rate limits: Reduce batch sizes and increase delays\n",
    "   - Quota issues: Check your Azure OpenAI usage limits\n",
    "   - Content filtering: Adjust domain topics if needed\n",
    "\n",
    "2. **AI Foundry Upload Issues**:\n",
    "   - Verify your AI Foundry project configuration\n",
    "   - Check authentication and permissions\n",
    "   - Ensure project endpoint is correct\n",
    "\n",
    "3. **Model Comparison Problems**:\n",
    "   - Start with smaller evaluation subsets\n",
    "   - Monitor token usage and costs\n",
    "   - Use appropriate delays between evaluations\n",
    "\n",
    "### Ready for Lab 3:\n",
    "In Lab 3, you'll learn to build **custom evaluators** that leverage the AI Foundry integration you've established, focusing on domain-specific evaluation logic and advanced evaluation patterns.\n",
    "\n",
    "### Production Deployment:\n",
    "Your Lab 2 evaluation framework provides the foundation for:\n",
    "- Continuous evaluation monitoring\n",
    "- A/B testing different models\n",
    "- Cost optimization strategies\n",
    "- Quality assurance workflows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}