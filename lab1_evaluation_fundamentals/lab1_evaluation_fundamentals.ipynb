{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: LLM Evaluation Fundamentals\n",
    "**Duration**: 60 minutes\n",
    "\n",
    "Welcome to the first lab of our LLM Evaluations Workshop! In this lab, you'll learn the fundamentals of evaluating Large Language Model outputs and understand why traditional software testing approaches don't work for LLMs.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this lab, you will:\n",
    "- Understand why LLM evaluation is critical for production systems\n",
    "- Learn core evaluation metrics (relevance, coherence, groundedness)\n",
    "- Perform your first evaluation using Azure AI Foundry SDK\n",
    "- Recognize the non-deterministic nature of LLM outputs\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Completed setup verification notebook\n",
    "- Azure OpenAI credentials configured\n",
    "- Understanding of basic programming concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introduction - The LLM Evaluation Problem (15 min)\n",
    "\n",
    "### Why Traditional Testing Doesn't Work for LLMs\n",
    "\n",
    "Traditional software testing relies on deterministic inputs and outputs:\n",
    "```python\n",
    "# Traditional function - always returns the same output\n",
    "def add_numbers(a, b):\n",
    "    return a + b\n",
    "\n",
    "assert add_numbers(2, 3) == 5  # This will always pass\n",
    "```\n",
    "\n",
    "LLMs are different - they're probabilistic and can generate different outputs for the same input!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dependencies imported successfully!\n",
      "Available demo prompts: ['factual', 'explanatory', 'creative', 'analytical', 'instruction']\n",
      "üè¢ Azure AI Foundry integration: ENABLED\n",
      "   ‚úÖ Evaluation results will appear in the AI Foundry portal\n"
     ]
    }
   ],
   "source": [
    "# Let's start by importing our dependencies\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to the path so we can import shared utilities\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd())))\n",
    "\n",
    "from shared_utils.azure_clients import azure_manager\n",
    "from shared_utils.foundry_evaluation import foundry_runner\n",
    "from lab1_evaluation_fundamentals.utils.lab1_helpers import (\n",
    "    demonstrate_llm_variability, \n",
    "    print_evaluation_insights,\n",
    "    DEMO_PROMPTS\n",
    ")\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Dependencies imported successfully!\")\n",
    "print(f\"Available demo prompts: {list(DEMO_PROMPTS.keys())}\")\n",
    "\n",
    "# Check AI Foundry integration status\n",
    "foundry_status = foundry_runner.get_status_info()\n",
    "if foundry_status['ai_foundry_available']:\n",
    "    print(\"üè¢ Azure AI Foundry integration: ENABLED\")\n",
    "    print(\"   ‚úÖ Evaluation results will appear in the AI Foundry portal\")\n",
    "else:\n",
    "    print(\"üîß Azure AI Foundry integration: DISABLED\")\n",
    "    print(\"   ‚ÑπÔ∏è To enable portal integration, add to .env:\")\n",
    "    print(\"      AZURE_AI_FOUNDRY_PROJECT_NAME, AZURE_AI_FOUNDRY_ENDPOINT, AZURE_AI_FOUNDRY_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: Same Prompt, Different Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ EXPERIMENT: Demonstrating LLM Non-Determinism\n",
      "We'll ask the same question 3 times and observe the variations...\n",
      "\n",
      "Running the same prompt 3 times to show variability...\n",
      "Prompt: What is the capital of Japan and what is it known for?\n",
      "--------------------------------------------------------------------------------\n",
      "Run 1:\n",
      "The capital of Japan is **Tokyo**.\n",
      "\n",
      "**Tokyo** is known for several things, including:\n",
      "\n",
      "- **Modernity and Technology:** It is renowned for its cutting-edge technology, vibrant cityscape, and innovative architecture.\n",
      "- **Cultural Heritage:** Tokyo blends traditional culture with modern life, featuring historic temples and shrines, such as Senso-ji and Meiji Shrine.\n",
      "- **Cuisine:** The city is famous for its diverse food scene, including sushi, ramen, and countless Michelin-starred restaurants.\n",
      "- **Fashion and Pop Culture:** Tokyo is a global center for fashion, anime, manga, and youth culture, especially in areas like Harajuku and Akihabara.\n",
      "- **Economic Power:** As one of the world's major financial centers\n",
      "--------------------------------------------------------------------------------\n",
      "Run 2:\n",
      "The capital of Japan is **Tokyo**.\n",
      "\n",
      "Tokyo is known for:\n",
      "- Being one of the world‚Äôs most populous and vibrant cities.\n",
      "- Its blend of ultra-modern skyscrapers and historic temples (such as Senso-ji).\n",
      "- Cutting-edge technology, fashion, and pop culture.\n",
      "- Iconic landmarks like the Tokyo Tower, Tokyo Skytree, and the Shibuya Crossing.\n",
      "- Excellent cuisine, including sushi, ramen, and izakaya dining.\n",
      "- Serving as the political, economic, and cultural center of Japan.\n",
      "- Hosting major events, such as the 2020 Summer Olympics (held in 2021 due to the pandemic).\n",
      "--------------------------------------------------------------------------------\n",
      "Run 3:\n",
      "The capital of **Japan** is **Tokyo**.\n",
      "\n",
      "**Tokyo** is known for:\n",
      "\n",
      "- Being one of the world‚Äôs largest and most populous cities.\n",
      "- Its blend of modern skyscrapers, cutting-edge technology, and historical sites like the Meiji Shrine and Senso-ji Temple.\n",
      "- Vibrant districts such as Shibuya (famous for its busy crossing), Shinjuku, and Akihabara (center of otaku culture).\n",
      "- World-class cuisine, including sushi, ramen, and a high concentration of Michelin-starred restaurants.\n",
      "- Hosting major events, including the 1964 and 2020 Summer Olympics.\n",
      "- Being a global center of finance, business, fashion, and pop culture.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üîç OBSERVATION:\n",
      "Notice how each response is different, even though we asked the exact same question!\n",
      "This is why we can't use traditional assert statements for LLM testing.\n"
     ]
    }
   ],
   "source": [
    "# Get Azure OpenAI client\n",
    "client = azure_manager.get_openai_client()\n",
    "deployment_name = os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME')\n",
    "\n",
    "# Demonstrate LLM variability with a factual question\n",
    "test_prompt = DEMO_PROMPTS['factual']\n",
    "\n",
    "print(\"üî¨ EXPERIMENT: Demonstrating LLM Non-Determinism\")\n",
    "print(\"We'll ask the same question 3 times and observe the variations...\\n\")\n",
    "\n",
    "responses = demonstrate_llm_variability(\n",
    "    client=client,\n",
    "    deployment_name=deployment_name,\n",
    "    prompt=test_prompt,\n",
    "    num_runs=3\n",
    ")\n",
    "\n",
    "print(\"\\nüîç OBSERVATION:\")\n",
    "print(\"Notice how each response is different, even though we asked the exact same question!\")\n",
    "print(\"This is why we can't use traditional assert statements for LLM testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Impact of Poor LLM Performance\n",
    "\n",
    "Poor LLM performance can lead to:\n",
    "- **Customer Dissatisfaction**: Irrelevant or unhelpful responses\n",
    "- **Brand Risk**: Inconsistent or inappropriate content generation\n",
    "- **Compliance Issues**: Incorrect information in regulated industries\n",
    "- **Cost Overruns**: Using expensive models when cheaper ones would suffice\n",
    "- **Security Risks**: Potential for harmful or biased outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Core Concepts - Understanding Evaluation Metrics (15 min)\n",
    "\n",
    "### Quality Metrics\n",
    "\n",
    "#### 1. Relevance\n",
    "- **Definition**: How well does the response address the question?\n",
    "- **Scale**: Typically 1-5 (1 = completely irrelevant, 5 = perfectly relevant)\n",
    "- **Use Case**: Ensuring responses actually answer the user's question\n",
    "\n",
    "#### 2. Coherence\n",
    "- **Definition**: How clear and logically structured is the response?\n",
    "- **Scale**: Typically 1-5 (1 = incoherent, 5 = perfectly coherent)\n",
    "- **Use Case**: Ensuring responses are understandable and well-organized\n",
    "\n",
    "#### 3. Fluency\n",
    "- **Definition**: How natural and grammatically correct is the language?\n",
    "- **Scale**: Typically 1-5 (1 = poor fluency, 5 = perfect fluency)\n",
    "- **Use Case**: Ensuring professional, readable output\n",
    "\n",
    "#### 4. Groundedness\n",
    "- **Definition**: How well is the response supported by the provided context?\n",
    "- **Scale**: Typically 1-5 (1 = not grounded, 5 = fully grounded)\n",
    "- **Use Case**: Preventing hallucination in RAG applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loaded 8 sample Q&A pairs\n",
      "\n",
      "üîç Sample data format:\n",
      "Query: What is the capital of France?\n",
      "Response: The capital of France is Paris.\n",
      "Context: France is a country in Western Europe. Its capital and largest city is Paris, which is also the coun...\n",
      "Ground Truth: Paris\n",
      "\n",
      "üí° Key Points:\n",
      "- Query: The question or prompt given to the LLM\n",
      "- Response: The LLM's answer\n",
      "- Context: Background information provided to the LLM\n",
      "- Ground Truth: The correct or expected answer (for reference)\n"
     ]
    }
   ],
   "source": [
    "# Let's examine our sample data to understand the format\n",
    "from shared_utils.evaluation_helpers import load_evaluation_data\n",
    "\n",
    "# Load our sample Q&A pairs\n",
    "sample_data = load_evaluation_data('data/sample_qa_pairs.jsonl')\n",
    "\n",
    "print(f\"üìä Loaded {len(sample_data)} sample Q&A pairs\")\n",
    "print(\"\\nüîç Sample data format:\")\n",
    "\n",
    "# Show the first example\n",
    "example = sample_data[0]\n",
    "print(f\"Query: {example['query']}\")\n",
    "print(f\"Response: {example['response']}\")\n",
    "print(f\"Context: {example['context'][:100]}...\")\n",
    "print(f\"Ground Truth: {example['ground_truth']}\")\n",
    "\n",
    "print(\"\\nüí° Key Points:\")\n",
    "print(\"- Query: The question or prompt given to the LLM\")\n",
    "print(\"- Response: The LLM's answer\")\n",
    "print(\"- Context: Background information provided to the LLM\")\n",
    "print(\"- Ground Truth: The correct or expected answer (for reference)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safety and Performance Metrics\n",
    "\n",
    "Beyond quality, we also evaluate:\n",
    "\n",
    "#### Safety Metrics\n",
    "- **Harmful Content**: Detection of potentially harmful outputs\n",
    "- **Bias Detection**: Identifying biased or discriminatory responses\n",
    "- **Toxicity**: Measuring offensive or inappropriate content\n",
    "\n",
    "#### Performance Metrics\n",
    "- **Latency**: Response time\n",
    "- **Token Usage**: Cost implications\n",
    "- **Throughput**: Requests per second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Hands-On - Basic Evaluation (25 min)\n",
    "\n",
    "Now let's perform our first LLM evaluation using Azure AI Foundry SDK!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Set Up the Evaluation Environment (with optional AI Foundry integration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä EVALUATION ENVIRONMENT SETUP\n",
      "==================================================\n",
      "üè¢ Azure AI Foundry Integration: ‚úÖ ENABLED\n",
      "   Your evaluation framework is configured for AI Foundry!\n",
      "   üì± Portal: https://ai.azure.com\n",
      "\n",
      "   ‚ÑπÔ∏è Current Status: azure-ai-projects v1.0.0 detected\n",
      "   üìä Evaluations run locally with enhanced metadata\n",
      "   üîÆ Future versions will automatically sync to portal\n",
      "\n",
      "üîß Current execution mode: enabled\n",
      "   Both modes provide identical evaluation capabilities!\n",
      "   üéØ The workshop works perfectly with or without AI Foundry integration\n"
     ]
    }
   ],
   "source": [
    "# Check AI Foundry integration status\n",
    "foundry_status = foundry_runner.get_status_info()\n",
    "print(\"üìä EVALUATION ENVIRONMENT SETUP\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if foundry_status['ai_foundry_available']:\n",
    "    print(\"üè¢ Azure AI Foundry Integration: ‚úÖ ENABLED\")\n",
    "    print(\"   Your evaluation framework is configured for AI Foundry!\")\n",
    "    print(\"   üì± Portal: https://ai.azure.com\")\n",
    "    print(\"\")\n",
    "    print(\"   ‚ÑπÔ∏è Current Status: azure-ai-projects v1.0.0 detected\")\n",
    "    print(\"   üìä Evaluations run locally with enhanced metadata\")\n",
    "    print(\"   üîÆ Future versions will automatically sync to portal\")\n",
    "else:\n",
    "    print(\"üîß Azure AI Foundry Integration: ‚ùå DISABLED\")\n",
    "    print(\"   Evaluations will run locally (fully functional)\")\n",
    "    print(\"\")\n",
    "    print(\"üí° To enable AI Foundry integration, add to your .env file:\")\n",
    "    print(\"   AZURE_AI_FOUNDRY_PROJECT_NAME=your-project-name\")\n",
    "    print(\"   AZURE_AI_FOUNDRY_ENDPOINT=https://your-project.cognitiveservices.azure.com/\")\n",
    "    print(\"   AZURE_AI_FOUNDRY_API_KEY=your-api-key\")\n",
    "    print(\"\")\n",
    "    print(\"üìö How to get these values:\")\n",
    "    print(\"   1. Go to https://ai.azure.com\")\n",
    "    print(\"   2. Create or select an AI Foundry project\")\n",
    "    print(\"   3. Go to Project Settings ‚Üí Keys and Endpoints\")\n",
    "    print(\"   4. Copy the endpoint URL and primary key\")\n",
    "\n",
    "print(f\"\\nüîß Current execution mode: {foundry_status['portal_integration']}\")\n",
    "print(\"   Both modes provide identical evaluation capabilities!\")\n",
    "print(\"   üéØ The workshop works perfectly with or without AI Foundry integration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluation environment set up!\n",
      "Model config: gpt-4.1\n"
     ]
    }
   ],
   "source": [
    "# Import evaluation components\n",
    "from azure.ai.evaluation import evaluate, RelevanceEvaluator, CoherenceEvaluator, FluencyEvaluator, GroundednessEvaluator\n",
    "\n",
    "# Get model configuration for evaluators\n",
    "model_config = azure_manager.get_model_config()\n",
    "\n",
    "print(\"‚úÖ Evaluation environment set up!\")\n",
    "print(f\"Model config: {model_config['azure_deployment']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Creating evaluators...\n",
      "‚úÖ Relevance evaluator created\n",
      "‚úÖ Coherence evaluator created\n",
      "‚úÖ Fluency evaluator created\n",
      "‚úÖ Groundedness evaluator created\n",
      "\n",
      "üìä Successfully created 4 evaluators: ['relevance', 'coherence', 'fluency', 'groundedness']\n"
     ]
    }
   ],
   "source": [
    "# Create individual evaluators\n",
    "print(\"üîß Creating evaluators...\")\n",
    "\n",
    "try:\n",
    "    relevance_evaluator = RelevanceEvaluator(model_config=model_config)\n",
    "    print(\"‚úÖ Relevance evaluator created\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to create relevance evaluator: {e}\")\n",
    "    relevance_evaluator = None\n",
    "\n",
    "try:\n",
    "    coherence_evaluator = CoherenceEvaluator(model_config=model_config)\n",
    "    print(\"‚úÖ Coherence evaluator created\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to create coherence evaluator: {e}\")\n",
    "    coherence_evaluator = None\n",
    "\n",
    "try:\n",
    "    fluency_evaluator = FluencyEvaluator(model_config=model_config)\n",
    "    print(\"‚úÖ Fluency evaluator created\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to create fluency evaluator: {e}\")\n",
    "    fluency_evaluator = None\n",
    "\n",
    "try:\n",
    "    groundedness_evaluator = GroundednessEvaluator(model_config=model_config)\n",
    "    print(\"‚úÖ Groundedness evaluator created\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to create groundedness evaluator: {e}\")\n",
    "    groundedness_evaluator = None\n",
    "\n",
    "# Create evaluators dictionary (only include successful ones)\n",
    "evaluators = {}\n",
    "if relevance_evaluator:\n",
    "    evaluators[\"relevance\"] = relevance_evaluator\n",
    "if coherence_evaluator:\n",
    "    evaluators[\"coherence\"] = coherence_evaluator\n",
    "if fluency_evaluator:\n",
    "    evaluators[\"fluency\"] = fluency_evaluator\n",
    "if groundedness_evaluator:\n",
    "    evaluators[\"groundedness\"] = groundedness_evaluator\n",
    "\n",
    "print(f\"\\nüìä Successfully created {len(evaluators)} evaluators: {list(evaluators.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Prepare Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Running evaluation on 3 examples...\n",
      "\n",
      "Example 1:\n",
      "  Query: What is the capital of France?\n",
      "  Response: The capital of France is Paris....\n",
      "  Has context: Yes\n",
      "\n",
      "Example 2:\n",
      "  Query: How do you calculate the area of a circle?\n",
      "  Response: The area of a circle is calculated using the formula A = œÄr¬≤, where r is the radius of the circle....\n",
      "  Has context: Yes\n",
      "\n",
      "Example 3:\n",
      "  Query: What is machine learning?\n",
      "  Response: Machine learning is a subset of artificial intelligence that enables computers to learn and make dec...\n",
      "  Has context: Yes\n"
     ]
    }
   ],
   "source": [
    "# Take a small subset for our first evaluation\n",
    "evaluation_data = sample_data[:3]  # Use first 3 examples\n",
    "\n",
    "print(f\"üî¨ Running evaluation on {len(evaluation_data)} examples...\")\n",
    "\n",
    "# Show what we're evaluating\n",
    "for i, item in enumerate(evaluation_data):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Query: {item['query']}\")\n",
    "    print(f\"  Response: {item['response'][:100]}...\")\n",
    "    print(f\"  Has context: {'Yes' if item.get('context') else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run the Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running evaluation...\n",
      "This may take a few minutes as we call the Azure OpenAI service for each metric...\n",
      "‚ùå Evaluation failed: (UserError) Unable to load data from './data/lab1_basic_evaluation.json'. Supported formats are JSONL and CSV. Detailed error: Expected object or value.\n",
      "\n",
      "üîß Troubleshooting tips:\n",
      "1. Check your Azure OpenAI credentials in .env\n",
      "2. Verify your deployment name is correct\n",
      "3. Ensure you have quota available in your Azure subscription\n",
      "4. Check if your endpoint is accessible\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.evaluation import evaluate, AzureAIProject\n",
    "\n",
    "# Run the evaluation (with optional Azure AI Foundry integration)\n",
    "print(\"üöÄ Running evaluation...\")\n",
    "print(\"This may take a few minutes as we call the Azure OpenAI service for each metric...\")\n",
    "\n",
    "try:\n",
    "    client = AIProjectClient(\n",
    "        endpoint=\"https://aifoundry825233136833-resource.services.ai.azure.com/api/projects/aifoundry825233136833\",\n",
    "        credential=DefaultAzureCredential()\n",
    "    )\n",
    "\n",
    "    # Use the enhanced foundry evaluation runner\n",
    "    # results = foundry_runner.run_evaluation(\n",
    "    #     data=evaluation_data,\n",
    "    #     evaluators=evaluators,\n",
    "    #     run_name=f\"Lab 1 Basic Evaluation - {len(evaluation_data)} items\",\n",
    "    #     description=f\"Foundational evaluation with {len(evaluators)} quality metrics\"\n",
    "    # )\n",
    "    proj = AzureAIProject(\n",
    "        subscription_id=os.environ[\"AZURE_SUBSCRIPTION_ID\"],\n",
    "        resource_group_name=os.environ[\"AZURE_RESOURCE_GROUP_NAME\"],\n",
    "        project_name=os.environ[\"AZURE_AI_FOUNDRY_PROJECT_NAME\"],\n",
    "    )\n",
    "    results = evaluate(\n",
    "        data='./data/sample_qa_pairs.jsonl',\n",
    "        evaluators=evaluators,\n",
    "        project=proj,\n",
    "    )\n",
    "    print(\"‚úÖ Evaluation completed successfully!\")\n",
    "    \n",
    "    # # Show execution method and results\n",
    "    # execution_method = results.get('_execution_method', 'unknown')\n",
    "    \n",
    "    # if execution_method == 'azure_ai_foundry_hybrid':\n",
    "    #     print(\"üè¢ Azure AI Foundry Integration: ACTIVE\")\n",
    "    #     print(f\"   üìÅ Dataset uploaded to AI Foundry portal\")\n",
    "    #     print(f\"   üìä Dataset ID: {results.get('_dataset_id', 'N/A')}\")\n",
    "    #     print(f\"   üìù Dataset name: {results.get('_dataset_name', 'N/A')}\")\n",
    "    #     print(f\"   üëÄ View at: https://ai.azure.com\")\n",
    "    #     print(f\"   üí° {results.get('_note', 'N/A')}\")\n",
    "    #     if 'metrics' in results:\n",
    "    #         print(\"üìà Evaluation metrics calculated successfully\")\n",
    "            \n",
    "    # elif execution_method == 'azure_ai_foundry_ready':\n",
    "    #     print(\"üè¢ Evaluation completed with AI Foundry integration prepared\")\n",
    "    #     print(f\"   üìä Run name: {results.get('_run_name', 'N/A')}\")\n",
    "    #     print(f\"   üìù Description: {results.get('_description', 'N/A')}\")\n",
    "    #     print(f\"   üîÆ Note: {results.get('_note', 'N/A')}\")\n",
    "    #     if 'metrics' in results:\n",
    "    #         print(\"üìà Metrics calculated successfully\")\n",
    "            \n",
    "    # elif execution_method == 'azure_ai_foundry':\n",
    "    #     print(\"üè¢ Results submitted to Azure AI Foundry portal\")\n",
    "    #     print(f\"   üìä Evaluation ID: {results.get('evaluation_id', 'N/A')}\")\n",
    "    #     print(f\"   üìÅ Dataset ID: {results.get('dataset_id', 'N/A')}\")\n",
    "    #     print(f\"   üëÄ View results at: {results.get('portal_url', 'https://ai.azure.com')}\")\n",
    "    #     print(\"   ‚è≥ Note: Results may take a few minutes to appear in the portal\")\n",
    "        \n",
    "    # else:\n",
    "    #     print(\"üîß Results processed locally\")\n",
    "    #     print(f\"üìä Evaluated {len(evaluation_data)} examples\")\n",
    "    #     if 'metrics' in results:\n",
    "    #         print(\"üìà Metrics calculated successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Evaluation failed: {e}\")\n",
    "    print(\"\\nüîß Troubleshooting tips:\")\n",
    "    print(\"1. Check your Azure OpenAI credentials in .env\")\n",
    "    print(\"2. Verify your deployment name is correct\")\n",
    "    print(\"3. Ensure you have quota available in your Azure subscription\")\n",
    "    print(\"4. Check if your endpoint is accessible\")\n",
    "    results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Interpret the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.30.1\n",
      "üìÅ Loading results from data/lab1_basic_evaluation.json\n",
      "‚úÖ Results loaded successfully!\n",
      "üîç EVALUATION INSIGHTS\n",
      "==================================================\n",
      "üìä Overall Metrics:\n",
      "  ‚Ä¢ relevance.relevance: 4.000\n",
      "    ‚Üí Excellent - Response directly addresses the question\n",
      "  ‚Ä¢ relevance.gpt_relevance: 4.000\n",
      "    ‚Üí Excellent - Response directly addresses the question\n",
      "  ‚Ä¢ relevance.relevance_threshold: 3.000\n",
      "    ‚Üí Good - Response is mostly relevant with minor issues\n",
      "  ‚Ä¢ coherence.coherence: 4.000\n",
      "    ‚Üí Excellent - Response is very clear and well-structured\n",
      "  ‚Ä¢ coherence.gpt_coherence: 4.000\n",
      "    ‚Üí Excellent - Response is very clear and well-structured\n",
      "  ‚Ä¢ coherence.coherence_threshold: 3.000\n",
      "    ‚Üí Good - Response is mostly coherent with good flow\n",
      "  ‚Ä¢ fluency.fluency: 3.667\n",
      "    ‚Üí Good - Response flows well with minor language issues\n",
      "  ‚Ä¢ fluency.gpt_fluency: 3.667\n",
      "    ‚Üí Good - Response flows well with minor language issues\n",
      "  ‚Ä¢ fluency.fluency_threshold: 3.000\n",
      "    ‚Üí Good - Response flows well with minor language issues\n",
      "  ‚Ä¢ groundedness.groundedness: 4.000\n",
      "    ‚Üí Excellent - Response is fully supported by the provided context\n",
      "  ‚Ä¢ groundedness.gpt_groundedness: 4.000\n",
      "    ‚Üí Excellent - Response is fully supported by the provided context\n",
      "  ‚Ä¢ groundedness.groundedness_threshold: 3.000\n",
      "    ‚Üí Good - Response is mostly grounded in the context\n",
      "  ‚Ä¢ relevance.binary_aggregate: 1.000\n",
      "    ‚Üí Poor - Response doesn't adequately address the question\n",
      "  ‚Ä¢ coherence.binary_aggregate: 1.000\n",
      "    ‚Üí Poor - Response lacks clear structure or logical flow\n",
      "  ‚Ä¢ fluency.binary_aggregate: 1.000\n",
      "    ‚Üí Poor - Response has significant language or grammar problems\n",
      "  ‚Ä¢ groundedness.binary_aggregate: 1.000\n",
      "    ‚Üí Poor - Response contains claims not supported by the context\n",
      "\n",
      "üìà Individual Results (showing first 3 of 3):\n",
      "\n",
      "  Result 1:\n",
      "\n",
      "  Result 2:\n",
      "\n",
      "  Result 3:\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import ipykernel\n",
    "\n",
    "ipykernel.__version__\n",
    "print(ipykernel.__version__)\n",
    "results_file = \"data/lab1_basic_evaluation.json\"\n",
    "\n",
    "if os.path.exists(results_file):\n",
    "    print(f\"üìÅ Loading results from {results_file}\")\n",
    "    with open(results_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    print(\"‚úÖ Results loaded successfully!\")\n",
    "\n",
    "    # Use our helper function to display insights\n",
    "    print_evaluation_insights(results)\n",
    "\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Results file not found at {results_file}\")\n",
    "    print(\"Make sure you've run the evaluation in the previous step first.\")\n",
    "    print(\"If evaluation failed, that's okay - the key learning is understanding the process.\")\n",
    "    results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Your Results\n",
    "\n",
    "Let's break down what these numbers mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö UNDERSTANDING YOUR EVALUATION RESULTS\n",
      "==================================================\n",
      "\n",
      "üéØ Overall Average Scores:\n",
      "\n",
      "RELEVANCE: 4.000\n",
      "  üìà EXCELLENT - Your LLM responses are performing very well!\n",
      "\n",
      "COHERENCE: 4.000\n",
      "  üìà EXCELLENT - Your LLM responses are performing very well!\n",
      "\n",
      "FLUENCY: 3.667\n",
      "  üìä GOOD - Solid performance with room for improvement\n",
      "\n",
      "GROUNDEDNESS: 4.000\n",
      "  üìà EXCELLENT - Your LLM responses are performing very well!\n",
      "\n",
      "üìã Individual Results (3 examples):\n",
      "========================================\n",
      "\n",
      "Example 1:\n",
      "  Query: What is the capital of France?\n",
      "  Scores:\n",
      "    ‚Ä¢ Relevance: 4.0/5.0\n",
      "    ‚Ä¢ Coherence: 4.0/5.0\n",
      "    ‚Ä¢ Fluency: 3.0/5.0\n",
      "    ‚Ä¢ Groundedness: 4.0/5.0\n",
      "  Key Insights:\n",
      "    ‚Ä¢ Relevance: The response directly and accurately answers the query by stating that Paris is ...\n",
      "    ‚Ä¢ Coherence: The response is fully coherent, directly answers the question, and is logically ...\n",
      "\n",
      "Example 2:\n",
      "  Query: How do you calculate the area of a circle?\n",
      "  Scores:\n",
      "    ‚Ä¢ Relevance: 4.0/5.0\n",
      "    ‚Ä¢ Coherence: 4.0/5.0\n",
      "    ‚Ä¢ Fluency: 4.0/5.0\n",
      "    ‚Ä¢ Groundedness: 5.0/5.0\n",
      "  Key Insights:\n",
      "    ‚Ä¢ Relevance: The response directly and accurately provides the formula for calculating the ar...\n",
      "    ‚Ä¢ Coherence: The response is coherent, logically organized, and clearly explains how to calcu...\n",
      "\n",
      "Example 3:\n",
      "  Query: What is machine learning?\n",
      "  Scores:\n",
      "    ‚Ä¢ Relevance: 4.0/5.0\n",
      "    ‚Ä¢ Coherence: 4.0/5.0\n",
      "    ‚Ä¢ Fluency: 4.0/5.0\n",
      "    ‚Ä¢ Groundedness: 3.0/5.0\n",
      "  Key Insights:\n",
      "    ‚Ä¢ Relevance: The response directly and accurately defines machine learning, specifying its re...\n",
      "    ‚Ä¢ Coherence: The response is coherent, logically organized, and clearly explains what machine...\n",
      "\n",
      "üí° Key Insights:\n",
      "‚Ä¢ Scores are on a 1-5 scale (5 being the best)\n",
      "‚Ä¢ Consistency across metrics indicates well-balanced responses\n",
      "‚Ä¢ Low groundedness might indicate hallucination issues\n",
      "‚Ä¢ Low relevance suggests the model isn't understanding the query well\n"
     ]
    }
   ],
   "source": [
    "if results and 'metrics' in results:\n",
    "    print(\"üìö UNDERSTANDING YOUR EVALUATION RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    metrics = results['metrics']\n",
    "\n",
    "    print(\"\\nüéØ Overall Average Scores:\")\n",
    "\n",
    "    # Extract the main metric scores (not the thresholds or binary aggregates)\n",
    "    main_metrics = {}\n",
    "    for metric_name, score in metrics.items():\n",
    "        if not any(x in metric_name for x in ['threshold', 'binary_aggregate', 'gpt_']):\n",
    "            # Get the base metric name (e.g., 'relevance' from 'relevance.relevance')\n",
    "            base_name = metric_name.split('.')[0]\n",
    "            if base_name not in main_metrics:\n",
    "                main_metrics[base_name] = score\n",
    "\n",
    "    for metric_name, score in main_metrics.items():\n",
    "        print(f\"\\n{metric_name.upper()}: {score:.3f}\")\n",
    "\n",
    "        if score >= 4.0:\n",
    "            print(\"  üìà EXCELLENT - Your LLM responses are performing very well!\")\n",
    "        elif score >= 3.0:\n",
    "            print(\"  üìä GOOD - Solid performance with room for improvement\")\n",
    "        elif score >= 2.0:\n",
    "            print(\"  üìâ FAIR - Some issues that should be addressed\")\n",
    "        else:\n",
    "            print(\"  üö® POOR - Significant improvement needed\")\n",
    "\n",
    "    # Show individual results\n",
    "    if 'rows' in results:\n",
    "        print(f\"\\nüìã Individual Results ({len(results['rows'])} examples):\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        for i, row in enumerate(results['rows']):\n",
    "            print(f\"\\nExample {i+1}:\")\n",
    "\n",
    "            # Show the query\n",
    "            query = row.get('inputs.query', 'Unknown query')\n",
    "            print(f\"  Query: {query}\")\n",
    "\n",
    "            # Show the scores for this example\n",
    "            scores = {}\n",
    "            for key, value in row.items():\n",
    "                if key.startswith('outputs.') and key.endswith('.relevance'):\n",
    "                    scores['Relevance'] = value\n",
    "                elif key.startswith('outputs.') and key.endswith('.coherence'):\n",
    "                    scores['Coherence'] = value\n",
    "                elif key.startswith('outputs.') and key.endswith('.fluency'):\n",
    "                    scores['Fluency'] = value\n",
    "                elif key.startswith('outputs.') and key.endswith('.groundedness'):\n",
    "                    scores['Groundedness'] = value\n",
    "\n",
    "            print(\"  Scores:\")\n",
    "            for metric, score in scores.items():\n",
    "                print(f\"    ‚Ä¢ {metric}: {score:.1f}/5.0\")\n",
    "\n",
    "            # Show the reasoning if available\n",
    "            reasons = {}\n",
    "            for key, value in row.items():\n",
    "                if '_reason' in key:\n",
    "                    metric_name = key.split('.')[1].title()\n",
    "                    reasons[metric_name] = value\n",
    "\n",
    "            if reasons:\n",
    "                print(\"  Key Insights:\")\n",
    "                for metric, reason in list(reasons.items())[:2]:  # Show first 2 to avoid clutter\n",
    "                    print(f\"    ‚Ä¢ {metric}: {reason[:80]}...\")\n",
    "\n",
    "    print(\"\\nüí° Key Insights:\")\n",
    "    print(\"‚Ä¢ Scores are on a 1-5 scale (5 being the best)\")\n",
    "    print(\"‚Ä¢ Consistency across metrics indicates well-balanced responses\")\n",
    "    print(\"‚Ä¢ Low groundedness might indicate hallucination issues\")\n",
    "    print(\"‚Ä¢ Low relevance suggests the model isn't understanding the query well\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nüìö CONCEPTUAL UNDERSTANDING\")\n",
    "    print(\"Even without live results, you now understand:\")\n",
    "    print(\"‚úÖ How to set up Azure AI evaluation frameworks\")\n",
    "    print(\"‚úÖ The key metrics for evaluating LLM responses\")\n",
    "    print(\"‚úÖ How to interpret evaluation scores\")\n",
    "    print(\"‚úÖ Why evaluation is critical for production LLM systems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç EVALUATION INSIGHTS\n",
      "==================================================\n",
      "üìä Overall Metrics:\n",
      "  ‚Ä¢ relevance.relevance: 4.000\n",
      "    ‚Üí Excellent - Response directly addresses the question\n",
      "  ‚Ä¢ relevance.gpt_relevance: 4.000\n",
      "    ‚Üí Excellent - Response directly addresses the question\n",
      "  ‚Ä¢ relevance.relevance_threshold: 3.000\n",
      "    ‚Üí Good - Response is mostly relevant with minor issues\n",
      "  ‚Ä¢ coherence.coherence: 4.000\n",
      "    ‚Üí Excellent - Response is very clear and well-structured\n",
      "  ‚Ä¢ coherence.gpt_coherence: 4.000\n",
      "    ‚Üí Excellent - Response is very clear and well-structured\n",
      "  ‚Ä¢ coherence.coherence_threshold: 3.000\n",
      "    ‚Üí Good - Response is mostly coherent with good flow\n",
      "  ‚Ä¢ fluency.fluency: 3.667\n",
      "    ‚Üí Good - Response flows well with minor language issues\n",
      "  ‚Ä¢ fluency.gpt_fluency: 3.667\n",
      "    ‚Üí Good - Response flows well with minor language issues\n",
      "  ‚Ä¢ fluency.fluency_threshold: 3.000\n",
      "    ‚Üí Good - Response flows well with minor language issues\n",
      "  ‚Ä¢ groundedness.groundedness: 4.000\n",
      "    ‚Üí Excellent - Response is fully supported by the provided context\n",
      "  ‚Ä¢ groundedness.gpt_groundedness: 4.000\n",
      "    ‚Üí Excellent - Response is fully supported by the provided context\n",
      "  ‚Ä¢ groundedness.groundedness_threshold: 3.000\n",
      "    ‚Üí Good - Response is mostly grounded in the context\n",
      "  ‚Ä¢ relevance.binary_aggregate: 1.000\n",
      "    ‚Üí Poor - Response doesn't adequately address the question\n",
      "  ‚Ä¢ coherence.binary_aggregate: 1.000\n",
      "    ‚Üí Poor - Response lacks clear structure or logical flow\n",
      "  ‚Ä¢ fluency.binary_aggregate: 1.000\n",
      "    ‚Üí Poor - Response has significant language or grammar problems\n",
      "  ‚Ä¢ groundedness.binary_aggregate: 1.000\n",
      "    ‚Üí Poor - Response contains claims not supported by the context\n",
      "\n",
      "üìà Individual Results (showing first 3 of 3):\n",
      "\n",
      "  Result 1:\n",
      "\n",
      "  Result 2:\n",
      "\n",
      "  Result 3:\n",
      "‚úÖ Results saved to: /Users/jinle/Repos/_Demos/llm-evaluations-workshop/lab1_evaluation_fundamentals/utils/../data/lab1_basic_evaluation.json\n"
     ]
    }
   ],
   "source": [
    "if results:\n",
    "    # Use our helper function to display insights\n",
    "    print_evaluation_insights(results)\n",
    "    \n",
    "    # Save results for later analysis\n",
    "    from lab1_evaluation_fundamentals.utils.lab1_helpers import save_lab1_results\n",
    "    save_lab1_results(results, \"lab1_basic_evaluation.json\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results to display - evaluation may have failed.\")\n",
    "    print(\"Don't worry! This is common in workshop environments.\")\n",
    "    print(\"The key learning is understanding the evaluation process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Exploring Different Types of Responses (Bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ EXPERIMENT: Comparing Response Quality\n",
      "Let's see how different response qualities are scored...\n",
      "\n",
      "Test Case 1: High - Direct, accurate, relevant\n",
      "Response: The capital of France is Paris. Paris is located in the north-central part of France and is the country's largest city.\n",
      "------------------------------------------------------------\n",
      "Test Case 2: Low - Doesn't answer the question\n",
      "Response: Well, France has many beautiful cities, and I think you might be interested in learning about French culture and cuisine. The Eiffel Tower is very famous.\n",
      "------------------------------------------------------------\n",
      "Test Case 3: Medium - Correct but adds irrelevant info\n",
      "Response: The capital is Paris and also France has a population of 67 million people living in cities like Lyon and Marseille which are also important economic centers.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Let's create some intentionally different quality responses to see how evaluation works\n",
    "test_cases = [\n",
    "    {\n",
    "        \"query\": \"What is the capital of France?\",\n",
    "        \"response\": \"The capital of France is Paris. Paris is located in the north-central part of France and is the country's largest city.\",\n",
    "        \"context\": \"France is a country in Western Europe. Its capital and largest city is Paris.\",\n",
    "        \"expected_quality\": \"High - Direct, accurate, relevant\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is the capital of France?\",\n",
    "        \"response\": \"Well, France has many beautiful cities, and I think you might be interested in learning about French culture and cuisine. The Eiffel Tower is very famous.\",\n",
    "        \"context\": \"France is a country in Western Europe. Its capital and largest city is Paris.\",\n",
    "        \"expected_quality\": \"Low - Doesn't answer the question\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is the capital of France?\",\n",
    "        \"response\": \"The capital is Paris and also France has a population of 67 million people living in cities like Lyon and Marseille which are also important economic centers.\",\n",
    "        \"context\": \"France is a country in Western Europe. Its capital and largest city is Paris.\",\n",
    "        \"expected_quality\": \"Medium - Correct but adds irrelevant info\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üß™ EXPERIMENT: Comparing Response Quality\")\n",
    "print(\"Let's see how different response qualities are scored...\\n\")\n",
    "\n",
    "for i, case in enumerate(test_cases):\n",
    "    print(f\"Test Case {i+1}: {case['expected_quality']}\")\n",
    "    print(f\"Response: {case['response']}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Evaluating different response qualities...\n",
      "\n",
      "üè¢ Running evaluation through Azure AI Foundry...\n",
      "   ‚úÖ Results will appear in the AI Foundry portal\n",
      "üè¢ Running evaluation with Azure AI Foundry integration...\n",
      "üì§ Uploading dataset to AI Foundry: evaluation_dataset_756063988765806918\n",
      "‚úÖ Dataset uploaded successfully: azureai://accounts/aifoundry825233136833-resource/projects/aifoundry825233136833/data/evaluation_dataset_756063988765806918/versions/1.0\n",
      "üìä Running evaluation locally (AI Foundry evaluation API not yet available)\n",
      "2025-08-24 14:48:42 -0500 6251540480 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-08-24 14:48:42 -0500 6251540480 execution.bulk     INFO     Average execution time for completed lines: 1.27 seconds. Estimated time for incomplete lines: 2.54 seconds.\n",
      "2025-08-24 14:48:42 -0500 6251540480 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-08-24 14:48:42 -0500 6251540480 execution.bulk     INFO     Average execution time for completed lines: 0.69 seconds. Estimated time for incomplete lines: 0.69 seconds.\n",
      "2025-08-24 14:48:42 -0500 6251540480 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-08-24 14:48:42 -0500 6251540480 execution.bulk     INFO     Average execution time for completed lines: 0.5 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-08-24 14:48:42 -0500 6285193216 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-08-24 14:48:42 -0500 6285193216 execution.bulk     INFO     Average execution time for completed lines: 1.71 seconds. Estimated time for incomplete lines: 3.42 seconds.\n",
      "2025-08-24 14:48:43 -0500 6302019584 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-08-24 14:48:43 -0500 6302019584 execution.bulk     INFO     Average execution time for completed lines: 1.87 seconds. Estimated time for incomplete lines: 3.74 seconds.\n",
      "2025-08-24 14:48:43 -0500 6285193216 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-08-24 14:48:43 -0500 6285193216 execution.bulk     INFO     Average execution time for completed lines: 0.96 seconds. Estimated time for incomplete lines: 0.96 seconds.\n",
      "2025-08-24 14:48:43 -0500 6285193216 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-08-24 14:48:43 -0500 6285193216 execution.bulk     INFO     Average execution time for completed lines: 0.66 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-08-24 14:48:43 -0500 6268366848 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-08-24 14:48:43 -0500 6268366848 execution.bulk     INFO     Average execution time for completed lines: 1.97 seconds. Estimated time for incomplete lines: 3.94 seconds.\n",
      "2025-08-24 14:48:43 -0500 6302019584 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-08-24 14:48:43 -0500 6302019584 execution.bulk     INFO     Average execution time for completed lines: 1.05 seconds. Estimated time for incomplete lines: 1.05 seconds.\n",
      "2025-08-24 14:48:43 -0500 6302019584 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-08-24 14:48:43 -0500 6302019584 execution.bulk     INFO     Average execution time for completed lines: 0.75 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-08-24 14:48:43 -0500 6268366848 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-08-24 14:48:43 -0500 6268366848 execution.bulk     INFO     Average execution time for completed lines: 1.18 seconds. Estimated time for incomplete lines: 1.18 seconds.\n",
      "2025-08-24 14:48:43 -0500 6268366848 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-08-24 14:48:43 -0500 6268366848 execution.bulk     INFO     Average execution time for completed lines: 0.83 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "‚úÖ Evaluation completed with dataset uploaded to AI Foundry\n",
      "üìÅ Dataset 'evaluation_dataset_756063988765806918' is now visible in AI Foundry portal\n",
      "üîÆ When evaluation API is available, it will use this uploaded dataset\n",
      "üìä COMPARISON RESULTS:\n",
      "========================================\n",
      "üè¢ Comparison dataset uploaded to AI Foundry\n",
      "   üìÅ Dataset ID: azureai://accounts/aifoundry825233136833-resource/projects/aifoundry825233136833/data/evaluation_dataset_756063988765806918/versions/1.0\n",
      "\n",
      "Case 1: High - Direct, accurate, relevant\n",
      "  Scores:\n",
      "    ‚Ä¢ Relevance: 5.0/5.0\n",
      "    ‚Ä¢ Coherence: 4.0/5.0\n",
      "    ‚Ä¢ Fluency: 4.0/5.0\n",
      "    ‚Ä¢ Groundedness: 3.0/5.0\n",
      "------------------------------\n",
      "\n",
      "Case 2: Low - Doesn't answer the question\n",
      "  Scores:\n",
      "    ‚Ä¢ Relevance: 2.0/5.0\n",
      "    ‚Ä¢ Coherence: 2.0/5.0\n",
      "    ‚Ä¢ Fluency: 3.0/5.0\n",
      "    ‚Ä¢ Groundedness: 3.0/5.0\n",
      "------------------------------\n",
      "\n",
      "Case 3: Medium - Correct but adds irrelevant info\n",
      "  Scores:\n",
      "    ‚Ä¢ Relevance: 5.0/5.0\n",
      "    ‚Ä¢ Coherence: 4.0/5.0\n",
      "    ‚Ä¢ Fluency: 3.0/5.0\n",
      "    ‚Ä¢ Groundedness: 3.0/5.0\n",
      "------------------------------\n",
      "\n",
      "üí° Notice how the scores reflect the expected quality differences!\n",
      "üè¢ Both evaluation datasets are now available in AI Foundry portal\n"
     ]
    }
   ],
   "source": [
    "# If we have working evaluators, let's test these cases\n",
    "if evaluators and len(evaluators) > 0:\n",
    "    print(\"üî¨ Evaluating different response qualities...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Use the enhanced foundry evaluation runner for comparison\n",
    "        comparison_results = foundry_runner.run_evaluation(\n",
    "            data=test_cases,\n",
    "            evaluators=evaluators,\n",
    "            run_name=\"Lab 1 Response Quality Comparison\",\n",
    "            description=\"Comparing high, medium, and low quality responses\"\n",
    "        )\n",
    "        \n",
    "        print(\"üìä COMPARISON RESULTS:\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Show execution method for comparison run\n",
    "        execution_method = comparison_results.get('_execution_method', 'unknown')\n",
    "        if execution_method == 'azure_ai_foundry_hybrid':\n",
    "            print(f\"üè¢ Comparison dataset uploaded to AI Foundry\")\n",
    "            print(f\"   üìÅ Dataset ID: {comparison_results.get('_dataset_id', 'N/A')}\")\n",
    "        \n",
    "        if 'rows' in comparison_results:\n",
    "            for i, (case, row) in enumerate(zip(test_cases, comparison_results['rows'])):\n",
    "                print(f\"\\nCase {i+1}: {case['expected_quality']}\")\n",
    "                \n",
    "                # Show the scores for this example\n",
    "                scores = {}\n",
    "                for key, value in row.items():\n",
    "                    if key.startswith('outputs.') and key.endswith('.relevance'):\n",
    "                        scores['Relevance'] = value\n",
    "                    elif key.startswith('outputs.') and key.endswith('.coherence'):\n",
    "                        scores['Coherence'] = value\n",
    "                    elif key.startswith('outputs.') and key.endswith('.fluency'):\n",
    "                        scores['Fluency'] = value\n",
    "                    elif key.startswith('outputs.') and key.endswith('.groundedness'):\n",
    "                        scores['Groundedness'] = value\n",
    "\n",
    "                print(\"  Scores:\")\n",
    "                for metric, score in scores.items():\n",
    "                    print(f\"    ‚Ä¢ {metric}: {score:.1f}/5.0\")\n",
    "                print(\"-\" * 30)\n",
    "        \n",
    "        print(\"\\nüí° Notice how the scores reflect the expected quality differences!\")\n",
    "        \n",
    "        if execution_method in ['azure_ai_foundry_hybrid', 'azure_ai_foundry']:\n",
    "            print(\"üè¢ Both evaluation datasets are now available in AI Foundry portal\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Comparison evaluation failed: {e}\")\n",
    "        print(\"This is common in workshop environments - the concept is what matters!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Evaluators not available for comparison test\")\n",
    "    print(\"But you can imagine how different quality responses would score differently!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Wrap-up and Key Takeaways (5 min)\n",
    "\n",
    "### üéØ What You've Learned\n",
    "\n",
    "1. **Why LLM Evaluation Matters**\n",
    "   - LLMs are non-deterministic (same input ‚â† same output)\n",
    "   - Traditional testing approaches don't work\n",
    "   - Quality directly impacts business outcomes\n",
    "\n",
    "2. **Core Evaluation Metrics**\n",
    "   - **Relevance**: Does it answer the question?\n",
    "   - **Coherence**: Is it clear and logical?\n",
    "   - **Fluency**: Is the language natural?\n",
    "   - **Groundedness**: Is it supported by context?\n",
    "\n",
    "3. **Practical Skills**\n",
    "   - Set up Azure AI evaluation environment\n",
    "   - Create and configure evaluators\n",
    "   - Run evaluations on sample data\n",
    "   - Interpret evaluation results\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "In **Lab 2**, you'll learn how to:\n",
    "- Scale evaluations to larger datasets\n",
    "- Generate synthetic evaluation data\n",
    "- Compare multiple models systematically\n",
    "- Analyze cost vs. quality trade-offs\n",
    "\n",
    "### üìù Practice Exercise (Optional)\n",
    "\n",
    "Try evaluating some of your own prompts and responses using the techniques you've learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ Congratulations! You've completed Lab 1: LLM Evaluation Fundamentals\n",
      "\n",
      "üìö Ready for Lab 2? Let's scale up your evaluation capabilities!\n"
     ]
    }
   ],
   "source": [
    "# Practice area - try your own evaluation!\n",
    "your_test_data = [\n",
    "    {\n",
    "        \"query\": \"YOUR QUESTION HERE\",\n",
    "        \"response\": \"YOUR LLM RESPONSE HERE\",\n",
    "        \"context\": \"ANY RELEVANT CONTEXT HERE (optional)\"\n",
    "    }\n",
    "    # Add more test cases as needed\n",
    "]\n",
    "\n",
    "# Uncomment and run to evaluate your own data\n",
    "# if evaluators:\n",
    "#     your_results = evaluate(data=your_test_data, evaluators=evaluators)\n",
    "#     print_evaluation_insights(your_results)\n",
    "\n",
    "print(\"üéâ Congratulations! You've completed Lab 1: LLM Evaluation Fundamentals\")\n",
    "print(\"\\nüìö Ready for Lab 2? Let's scale up your evaluation capabilities!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üÜò Troubleshooting\n",
    "\n",
    "**Common Issues and Solutions:**\n",
    "\n",
    "1. **Import Errors**: \n",
    "   - Run `pip install -r requirements.txt`\n",
    "   - Check that you're in the correct virtual environment\n",
    "\n",
    "2. **Azure Connection Issues**:\n",
    "   - Verify your `.env` file has correct credentials\n",
    "   - Check your Azure OpenAI deployment is active\n",
    "   - Ensure you have sufficient quota\n",
    "\n",
    "3. **Evaluation Failures**:\n",
    "   - This is common in workshop environments\n",
    "   - The learning objective is understanding the process\n",
    "   - Try with smaller datasets or single evaluators\n",
    "\n",
    "4. **Rate Limiting**:\n",
    "   - Add delays between requests\n",
    "   - Use smaller batch sizes\n",
    "   - Check your Azure OpenAI tier limits\n",
    "\n",
    "**Need Help?**\n",
    "- Check the troubleshooting guide: `docs/troubleshooting.md`\n",
    "- Open an issue in the GitHub repository\n",
    "- Review Azure AI documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "zw1klq6xti",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current evaluation_data structure:\n",
      "Type: <class 'list'>\n",
      "Length: 3\n",
      "First item keys: ['query', 'response', 'context', 'ground_truth']\n"
     ]
    }
   ],
   "source": [
    "# Let's examine the current values to understand the issue\n",
    "print(\"Current evaluation_data structure:\")\n",
    "if 'evaluation_data' in locals():\n",
    "    print(f\"Type: {type(evaluation_data)}\")\n",
    "    print(f\"Length: {len(evaluation_data)}\")\n",
    "    print(f\"First item keys: {list(evaluation_data[0].keys()) if evaluation_data else 'No data'}\")\n",
    "else:\n",
    "    print(\"evaluation_data not defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "tsxbu784q39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ print_evaluation_insights imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Check what's currently imported from lab1_helpers\n",
    "from lab1_evaluation_fundamentals.utils.lab1_helpers import print_evaluation_insights\n",
    "print(\"‚úÖ print_evaluation_insights imported successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-evals-workshop (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
